{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements for the following codings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zhenjiaodu/opt/anaconda3/envs/unidl/bin/pip\", line 4, in <module>\n",
      "    import re\n",
      "  File \"/Users/zhenjiaodu/opt/anaconda3/envs/unidl/lib/python3.9/re.py\", line 125, in <module>\n",
      "    import sre_compile\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 941, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1039, in get_data\n",
      "KeyboardInterrupt\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhenjiaodu/opt/anaconda3/envs/unidl/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.2.2-cp39-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, jinja2, fsspec, filelock, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "### packages required \n",
    "!pip install fair-esm \n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
    "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
    "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you conputer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  # load the model\n",
    "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
    "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "  token_representations = results[\"representations\"][6]\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading and embeddings (main dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset loading\n",
    "dataset = pd.read_excel('bitter_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "\n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "\n",
    "embeddings_results.to_csv('bitter_train_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "\n",
    "# loading the y dataset for model development \n",
    "y_train = dataset['label']\n",
    "y_train = np.array(y_train) # transformed as np.array for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset loading\n",
    "dataset = pd.read_excel('bitter_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "# embedding all the peptide one by one\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "\n",
    "embeddings_results.to_csv('bitter_test_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "# loading the y dataset for model development \n",
    "y_test = dataset['label']\n",
    "y_test = np.array(y_test) # transformed as np.array for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 320)\n",
      "(103, 320)\n",
      "(128, 320)\n",
      "(409,)\n",
      "(103,)\n",
      "(128,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['minmax_scaler.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training dataset loading\n",
    "dataset = pd.read_excel('bitter_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "# loading the y dataset for model development \n",
    "y_train = dataset['label']\n",
    "y_train = np.array(y_train) # transformed as np.array for CNN model\n",
    "# test dataset loading\n",
    "dataset = pd.read_excel('bitter_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "# loading the y dataset for model development \n",
    "y_test = dataset['label']\n",
    "y_test = np.array(y_test) # transformed as np.array for CNN model\n",
    "# assign the dataset \n",
    "X_train_data_name = 'bitter_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_test_data_name = 'bitter_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_train = np.array(X_train_data)\n",
    "X_test = np.array(X_test_data)\n",
    "\n",
    "# re-divide dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123,shuffle=True, stratify=y_train)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_sub)\n",
    "X_train_sub = scaler.transform(X_train_sub) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# check the dimension of the dataset before model development\n",
    "print(X_train_sub.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train_sub.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Save the scaler to a file\n",
    "import joblib\n",
    "joblib.dump(scaler, 'minmax_scaler.pkl')\n",
    "# # Load the scaler from the file\n",
    "# loaded_scaler = joblib.load('minmax_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 15:18:02.810322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense, Activation, BatchNormalization, Flatten, Conv1D,Dropout, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "import keras\n",
    "from keras import backend as K\n",
    "def ESM_CNN(X_train, y_train, X_valid, y_valid):\n",
    "  inputShape=(320,1)\n",
    "  input = Input(inputShape)\n",
    "  x = Conv1D(32,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "  model = Model(inputs = input,outputs = x,name='Predict')\n",
    "  # define SGD optimizer\n",
    "  momentum = 0.5\n",
    "  sgd = SGD(learning_rate=0.01, momentum=momentum, nesterov=False)\n",
    "  # compile the model\n",
    "  model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "  # learning deccay setting\n",
    "  import math\n",
    "  def step_decay(epoch): # gradually decrease the learning rate\n",
    "      initial_lrate=0.1\n",
    "      drop=0.6\n",
    "      epochs_drop = 3.0\n",
    "      lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "            math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "      return lrate\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "  # early stop setting\n",
    "  early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,restore_best_weights = True)\n",
    "  mc = ModelCheckpoint('best_model.keras',  monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "  # summary the callbacks_list\n",
    "  callbacks_list = [ lrate , early_stop, mc]\n",
    "  model_history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                            epochs=200, callbacks=callbacks_list,batch_size = 8, verbose=2)\n",
    "  return model, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import math\n",
    "def model_evaluation(model_name, X_test, y_test):\n",
    "    # Load the saved model\n",
    "    saved_model = load_model(model_name)\n",
    "    # Predict class probabilities\n",
    "    predicted_protability = saved_model.predict(X_test, batch_size=1) \n",
    "    predicted_class = np.argmax(predicted_protability, axis=1) # operating horizontally /// row-wise\n",
    "    # True labels\n",
    "    y_true = y_test\n",
    "    # Calculate confusion matrix components\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    # Calculate evaluation metrics\n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Sn = TP / (TP + FN)\n",
    "    Sp = TN / (TN + FP)\n",
    "    MCC = (TP * TN - FP * FN) / (math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))) \n",
    "    BACC = 0.5 * Sn + 0.5 * Sp\n",
    "    AUC = roc_auc_score(y_true, predicted_protability[:, 1])\n",
    "    # Print results\n",
    "    print(\"Accuracy:\", ACC)\n",
    "    print(\"Balanced Accuracy:\", BACC)\n",
    "    print(\"Sensitivity (Recall):\", Sn)\n",
    "    print(\"Specificity:\", Sp)\n",
    "    print(\"MCC:\", MCC)\n",
    "    print(\"AUC:\", AUC)\n",
    "    return ACC, BACC, Sn, Sp, MCC, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_history = ESM_CNN(X_train_sub, y_train_sub, X_val , y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 1s 2ms/step\n",
      "Accuracy: 0.9453125\n",
      "Balanced Accuracy: 0.9453125\n",
      "Sensitivity (Recall): 0.96875\n",
      "Specificity: 0.921875\n",
      "MCC: 0.8916050852754615\n",
      "AUC: 0.974853515625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9453125, 0.9453125, 0.96875, 0.921875, 0.8916050852754615, 0.974853515625)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation('best_model.keras',X_test,y_test)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_keras_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
