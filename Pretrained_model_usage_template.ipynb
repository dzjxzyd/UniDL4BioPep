{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### packages required\n",
        "!pip install fair-esm\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "nxpbE8_xphTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
        "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
        "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
      ],
      "metadata": {
        "id": "X89TBa0dqAbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def esm_embeddings(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long,\n",
        "  #         or you have too many sequences for transformation in a single converting,\n",
        "  #         you computer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2 = esm2.eval().to(device)\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter()\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][6].cpu()\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations\n",
        "  return embeddings_results\n"
      ],
      "metadata": {
        "id": "kZwyvcU_p8aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset loading\n",
        "Notice: you must load the train dataset of the pretrained model (we need it to normalize the new data for prediction after embeddings)"
      ],
      "metadata": {
        "id": "kUvPNqpFqXzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()"
      ],
      "metadata": {
        "id": "XqWDQVfEqXLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('sample_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('sample_train_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n"
      ],
      "metadata": {
        "id": "e5dzMsboqQzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate embeddings for unknown peptide and the normalize them based on the train dataset"
      ],
      "metadata": {
        "id": "Cg8TiuPUqzNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset loading\n",
        "dataset = pd.read_excel('sample_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('sample_test_test_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n"
      ],
      "metadata": {
        "id": "s9nhVrWyqD-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the dataset\n",
        "X_train_data_name = 'sample_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_test_data_name = 'sample_test_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_train = np.array(X_train_data)\n",
        "X_test = np.array(X_test_data)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "xbheJRyvqStd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict results based on available model\n",
        "\n",
        "An example for prediction\n",
        "```\n",
        "from keras.models import load_model\n",
        "model = load_model('content/DPPIV_tensorflow_model')\n",
        "predicted_class= []\n",
        "predicted_protability = model.predict(X_test,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "print(predicted_class)\n",
        "```"
      ],
      "metadata": {
        "id": "8JGrvyAzpwBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load GPU for accelerate if available\n",
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')\n"
      ],
      "metadata": {
        "id": "zahGV75UPp_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1lvBHhTpMg3"
      },
      "outputs": [],
      "source": [
        "# unzip the model folder\n",
        "# please change the name to your model zip filename\n",
        "!unzip model_zip_filename.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "from keras.models import load_model\n",
        "model = load_model('content/model_zip_filename')"
      ],
      "metadata": {
        "id": "wBXUDnrVpZcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the pretrained model for prediction\n",
        "predicted_class= []\n",
        "predicted_protability = model.predict(X_test,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "print(predicted_class)"
      ],
      "metadata": {
        "id": "-jOrBZsDrrG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##########################################################\n",
        "Take care, the prediction result (positive and negative) is based on the original dataset. Here, This example only\n",
        "##########################################################"
      ],
      "metadata": {
        "id": "vGR5ZKTCQJwQ"
      }
    }
  ]
}