{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### requirements for the following codings\n"
   ],
   "metadata": {
    "id": "95NTckuFZZzm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### packages required \n",
    "!pip install fair-esm \n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ],
   "metadata": {
    "id": "UO71IBS6ZgZV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
    "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
    "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
   ],
   "metadata": {
    "id": "m91cA0H5w_eY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you conputer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  # load the model\n",
    "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
    "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "  token_representations = results[\"representations\"][6]\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  return embeddings_results"
   ],
   "metadata": {
    "id": "pl7XVx5HZsHf"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### data loading and embeddings"
   ],
   "metadata": {
    "id": "RddxugbsdR1Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "n6NOFoREw-40"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# training dataset loading\n",
    "dataset = pd.read_excel('DPPIV_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "peptide_sequence_list = []\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "\n",
    "# employ ESM model for converting and save the converted data in csv format\n",
    "embeddings_results = esm_embeddings(peptide_sequence_list)\n",
    "embeddings_results.to_csv('DPPIV_train_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "\n",
    "# loading the y dataset for model development \n",
    "y_train = dataset['label']\n",
    "y_train = np.array(y_train) # transformed as np.array for CNN model"
   ],
   "metadata": {
    "id": "LNlD8pvizH84"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# test dataset loading\n",
    "dataset = pd.read_excel('DPPIV_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "peptide_sequence_list = []\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "\n",
    "# employ ESM model for converting and save the converted data in csv format\n",
    "embeddings_results = esm_embeddings(peptide_sequence_list)\n",
    "embeddings_results.to_csv('DPPIV_test_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "\n",
    "# loading the y dataset for model development \n",
    "y_test = dataset['label']\n",
    "y_test = np.array(y_test) # transformed as np.array for CNN model"
   ],
   "metadata": {
    "id": "U7jxoIsCw8dW"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# assign the dataset \n",
    "X_train_data_name = 'DPPIV_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_test_data_name = 'DPPIV_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_train = np.array(X_train_data)\n",
    "X_test = np.array(X_test_data)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
    "X_test = scaler.transform(X_test)"
   ],
   "metadata": {
    "id": "Xk13-JbBXAph"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# check the dimension of the dataset before model development\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HubTATKXslKw",
    "outputId": "19962eb9-0a91-4598-d37a-6109b402a80a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1064, 320)\n",
      "(266, 320)\n",
      "(1064,)\n",
      "(266,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model architecture"
   ],
   "metadata": {
    "id": "U3Fagh9Iw83q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def ESM_CNN(X_train, y_train, X_test, y_test):\n",
    "  from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D,Conv1D\n",
    "  from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, AveragePooling1D, MaxPooling1D\n",
    "  from keras.models import Sequential,Model\n",
    "  from keras.optimizers import SGD\n",
    "  from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
    "  import keras\n",
    "  from keras import backend as K\n",
    "  inputShape=(320,1)\n",
    "  input = Input(inputShape)\n",
    "  x = Conv1D(128,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Conv1D(32,(3),strides = (1),name='layer_conv2',padding='same')(input)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "  model = Model(inputs = input,outputs = x,name='Predict')\n",
    "  # define SGD optimizer\n",
    "  momentum = 0.5\n",
    "  sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
    "  # compile the model\n",
    "  model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "  # learning deccay setting\n",
    "  import math\n",
    "  def step_decay(epoch): # gradually decrease the learning rate \n",
    "      initial_lrate=0.1\n",
    "      drop=0.6\n",
    "      epochs_drop = 3.0\n",
    "      lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "            math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "      return lrate\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "  # early stop setting\n",
    "  early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,restore_best_weights = True)\n",
    "\n",
    "  # summary the callbacks_list\n",
    "  callbacks_list = [ lrate , early_stop]\n",
    "\n",
    "  model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                            epochs=200,callbacks=callbacks_list,batch_size = 8, verbose=1)\n",
    "  return model, model_history"
   ],
   "metadata": {
    "id": "b0QeK6-Cg_cv"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10-fold cross validation"
   ],
   "metadata": {
    "id": "sws_G8h08tuq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Implementing 10-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "k = 10 \n",
    "kf = KFold(n_splits=k, shuffle = True, random_state=1)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "# result collection list\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "\n",
    "for train_index , test_index in kf.split(y_train):\n",
    "    X_train_CV , X_valid_CV = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
    "    y_train_CV , y_valid_CV = y_train.iloc[train_index] , y_train.iloc[test_index]\n",
    "    model, model_history = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    # confusion matrix \n",
    "    predicted_class= []\n",
    "    predicted_protability = model.predict(X_valid_CV,batch_size=1)\n",
    "    for i in range(predicted_protability.shape[0]):\n",
    "      index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
    "      predicted_class.append(index)\n",
    "    predicted_class = np.array(predicted_class)\n",
    "    y_true = y_valid_CV    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import math\n",
    "    # np.ravel() return a flatten 1D array\n",
    "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "    ACC_collecton.append(ACC)\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    AUC = roc_auc_score(y_valid_CV, predicted_protability[:,1])\n",
    "    AUC_collecton.append(AUC)\n"
   ],
   "metadata": {
    "id": "iFGZ88goj6u4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from statistics import mean, stdev\n",
    "print(mean(ACC_collecton),'±',stdev(ACC_collecton))\n",
    "print(mean(BACC_collecton),'±',stdev(BACC_collecton))\n",
    "print(mean(Sn_collecton),'±',stdev(Sn_collecton))\n",
    "print(mean(Sp_collecton),'±',stdev(Sp_collecton))\n",
    "print(mean(MCC_collecton),'±',stdev(MCC_collecton))\n",
    "print(mean(AUC_collecton),'±',stdev(AUC_collecton))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTi2x37MzsIY",
    "outputId": "7592e92f-e788-4416-e3a9-1c5e8e9ecb1c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8523364485981308 ± 0.03833829966157558\n",
      "0.8543898003559302 ± 0.03916058959118413\n",
      "0.8375668511708247 ± 0.029250976449937266\n",
      "0.8712127495410356 ± 0.06323581270465184\n",
      "0.7065978933180121 ± 0.07841146828497925\n",
      "0.9333615809876247 ± 0.03227863772408223\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model evaluation in test dataset"
   ],
   "metadata": {
    "id": "5JBlTA9shnQE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# result collection list\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "model, model_history = ESM_CNN(X_train, y_train, X_test , y_test)\n",
    "# confusion matrix \n",
    "predicted_class= []\n",
    "predicted_protability = model.predict(X_test,batch_size=1)\n",
    "for i in range(predicted_protability.shape[0]):\n",
    "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
    "  predicted_class.append(index)\n",
    "predicted_class = np.array(predicted_class)\n",
    "y_true = y_test    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "# np.ravel() return a flatten 1D array\n",
    "TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "ACC_collecton.append(ACC)\n",
    "Sn_collecton.append(TP/(TP+FN))\n",
    "Sp_collecton.append(TN/(TN+FP))\n",
    "MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "MCC_collecton.append(MCC)\n",
    "BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
    "AUC_collecton.append(AUC)"
   ],
   "metadata": {
    "id": "KPwEv_WsnH6Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7cd9479a-65eb-4215-8956-ff93620ffcbf"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "133/133 [==============================] - 2s 10ms/step - loss: 0.8429 - accuracy: 0.6974 - val_loss: 0.7368 - val_accuracy: 0.5000 - lr: 0.1000\n",
      "Epoch 2/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.4583 - accuracy: 0.7566 - val_loss: 0.6887 - val_accuracy: 0.5113 - lr: 0.1000\n",
      "Epoch 3/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.3487 - accuracy: 0.8233 - val_loss: 0.5012 - val_accuracy: 0.7519 - lr: 0.0600\n",
      "Epoch 4/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.3264 - accuracy: 0.8308 - val_loss: 0.3688 - val_accuracy: 0.8120 - lr: 0.0600\n",
      "Epoch 5/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.3384 - accuracy: 0.8374 - val_loss: 0.4434 - val_accuracy: 0.7970 - lr: 0.0600\n",
      "Epoch 6/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.3034 - accuracy: 0.8543 - val_loss: 0.3384 - val_accuracy: 0.8158 - lr: 0.0360\n",
      "Epoch 7/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.2794 - accuracy: 0.8675 - val_loss: 0.3252 - val_accuracy: 0.8045 - lr: 0.0360\n",
      "Epoch 8/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.2715 - accuracy: 0.8675 - val_loss: 0.3767 - val_accuracy: 0.8083 - lr: 0.0360\n",
      "Epoch 9/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.2470 - accuracy: 0.8778 - val_loss: 0.3402 - val_accuracy: 0.8158 - lr: 0.0216\n",
      "Epoch 10/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.2423 - accuracy: 0.8712 - val_loss: 0.3237 - val_accuracy: 0.8195 - lr: 0.0216\n",
      "Epoch 11/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.2278 - accuracy: 0.8825 - val_loss: 0.3656 - val_accuracy: 0.8233 - lr: 0.0216\n",
      "Epoch 12/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.2238 - accuracy: 0.8891 - val_loss: 0.3685 - val_accuracy: 0.8195 - lr: 0.0130\n",
      "Epoch 13/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.2080 - accuracy: 0.8844 - val_loss: 0.3576 - val_accuracy: 0.8271 - lr: 0.0130\n",
      "Epoch 14/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.2132 - accuracy: 0.8947 - val_loss: 0.3432 - val_accuracy: 0.8271 - lr: 0.0130\n",
      "Epoch 15/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1985 - accuracy: 0.9041 - val_loss: 0.3525 - val_accuracy: 0.8346 - lr: 0.0078\n",
      "Epoch 16/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.2002 - accuracy: 0.9041 - val_loss: 0.3544 - val_accuracy: 0.8421 - lr: 0.0078\n",
      "Epoch 17/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1867 - accuracy: 0.9164 - val_loss: 0.3684 - val_accuracy: 0.8346 - lr: 0.0078\n",
      "Epoch 18/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1886 - accuracy: 0.9088 - val_loss: 0.3571 - val_accuracy: 0.8346 - lr: 0.0047\n",
      "Epoch 19/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1735 - accuracy: 0.9220 - val_loss: 0.3504 - val_accuracy: 0.8383 - lr: 0.0047\n",
      "Epoch 20/200\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.1727 - accuracy: 0.9192 - val_loss: 0.3511 - val_accuracy: 0.8534 - lr: 0.0047\n",
      "Epoch 21/200\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.1762 - accuracy: 0.9126 - val_loss: 0.3566 - val_accuracy: 0.8346 - lr: 0.0028\n",
      "Epoch 22/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1771 - accuracy: 0.9154 - val_loss: 0.3585 - val_accuracy: 0.8383 - lr: 0.0028\n",
      "Epoch 23/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1692 - accuracy: 0.9192 - val_loss: 0.3574 - val_accuracy: 0.8459 - lr: 0.0028\n",
      "Epoch 24/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1835 - accuracy: 0.9098 - val_loss: 0.3540 - val_accuracy: 0.8496 - lr: 0.0017\n",
      "Epoch 25/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1663 - accuracy: 0.9248 - val_loss: 0.3583 - val_accuracy: 0.8459 - lr: 0.0017\n",
      "Epoch 26/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1757 - accuracy: 0.9164 - val_loss: 0.3506 - val_accuracy: 0.8421 - lr: 0.0017\n",
      "Epoch 27/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1657 - accuracy: 0.9258 - val_loss: 0.3550 - val_accuracy: 0.8421 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1697 - accuracy: 0.9239 - val_loss: 0.3513 - val_accuracy: 0.8459 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1716 - accuracy: 0.9126 - val_loss: 0.3505 - val_accuracy: 0.8421 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1744 - accuracy: 0.9201 - val_loss: 0.3509 - val_accuracy: 0.8459 - lr: 6.0466e-04\n",
      "Epoch 31/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1682 - accuracy: 0.9201 - val_loss: 0.3512 - val_accuracy: 0.8459 - lr: 6.0466e-04\n",
      "Epoch 32/200\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.1602 - accuracy: 0.9305 - val_loss: 0.3543 - val_accuracy: 0.8496 - lr: 6.0466e-04\n",
      "Epoch 33/200\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.1694 - accuracy: 0.9276 - val_loss: 0.3543 - val_accuracy: 0.8421 - lr: 3.6280e-04\n",
      "Epoch 34/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1667 - accuracy: 0.9267 - val_loss: 0.3550 - val_accuracy: 0.8534 - lr: 3.6280e-04\n",
      "Epoch 35/200\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.1655 - accuracy: 0.9201 - val_loss: 0.3550 - val_accuracy: 0.8534 - lr: 3.6280e-04\n",
      "Epoch 36/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1609 - accuracy: 0.9211 - val_loss: 0.3557 - val_accuracy: 0.8534 - lr: 2.1768e-04\n",
      "Epoch 37/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1551 - accuracy: 0.9295 - val_loss: 0.3561 - val_accuracy: 0.8534 - lr: 2.1768e-04\n",
      "Epoch 38/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1644 - accuracy: 0.9220 - val_loss: 0.3554 - val_accuracy: 0.8534 - lr: 2.1768e-04\n",
      "Epoch 39/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1589 - accuracy: 0.9361 - val_loss: 0.3564 - val_accuracy: 0.8496 - lr: 1.3061e-04\n",
      "Epoch 40/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1642 - accuracy: 0.9286 - val_loss: 0.3564 - val_accuracy: 0.8496 - lr: 1.3061e-04\n",
      "Epoch 41/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1645 - accuracy: 0.9305 - val_loss: 0.3562 - val_accuracy: 0.8496 - lr: 1.3061e-04\n",
      "Epoch 42/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1619 - accuracy: 0.9323 - val_loss: 0.3569 - val_accuracy: 0.8496 - lr: 7.8364e-05\n",
      "Epoch 43/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1662 - accuracy: 0.9220 - val_loss: 0.3567 - val_accuracy: 0.8496 - lr: 7.8364e-05\n",
      "Epoch 44/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1597 - accuracy: 0.9305 - val_loss: 0.3569 - val_accuracy: 0.8534 - lr: 7.8364e-05\n",
      "Epoch 45/200\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.1650 - accuracy: 0.9267 - val_loss: 0.3565 - val_accuracy: 0.8496 - lr: 4.7018e-05\n",
      "Epoch 46/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1613 - accuracy: 0.9248 - val_loss: 0.3567 - val_accuracy: 0.8496 - lr: 4.7018e-05\n",
      "Epoch 47/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1635 - accuracy: 0.9192 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 4.7018e-05\n",
      "Epoch 48/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1540 - accuracy: 0.9333 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 2.8211e-05\n",
      "Epoch 49/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1656 - accuracy: 0.9192 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 2.8211e-05\n",
      "Epoch 50/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1606 - accuracy: 0.9286 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 2.8211e-05\n",
      "Epoch 51/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1537 - accuracy: 0.9361 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 1.6927e-05\n",
      "Epoch 52/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1711 - accuracy: 0.9182 - val_loss: 0.3564 - val_accuracy: 0.8534 - lr: 1.6927e-05\n",
      "Epoch 53/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1619 - accuracy: 0.9276 - val_loss: 0.3563 - val_accuracy: 0.8534 - lr: 1.6927e-05\n",
      "Epoch 54/200\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.1581 - accuracy: 0.9258 - val_loss: 0.3568 - val_accuracy: 0.8534 - lr: 1.0156e-05\n",
      "Epoch 55/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1720 - accuracy: 0.9267 - val_loss: 0.3569 - val_accuracy: 0.8534 - lr: 1.0156e-05\n",
      "Epoch 56/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1616 - accuracy: 0.9305 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 1.0156e-05\n",
      "Epoch 57/200\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.1626 - accuracy: 0.9248 - val_loss: 0.3566 - val_accuracy: 0.8534 - lr: 6.0936e-06\n",
      "Epoch 58/200\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.1676 - accuracy: 0.9258 - val_loss: 0.3567 - val_accuracy: 0.8534 - lr: 6.0936e-06\n",
      "Epoch 59/200\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.1741 - accuracy: 0.9164 - val_loss: 0.3568 - val_accuracy: 0.8534 - lr: 6.0936e-06\n",
      "Epoch 60/200\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.1582 - accuracy: 0.9258 - val_loss: 0.3568 - val_accuracy: 0.8534 - lr: 3.6562e-06\n",
      "266/266 [==============================] - 1s 2ms/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(ACC_collecton[0])\n",
    "print(BACC_collecton[0])\n",
    "print(Sn_collecton[0])\n",
    "print(Sp_collecton[0])\n",
    "print(MCC_collecton[0])\n",
    "print(AUC_collecton[0])"
   ],
   "metadata": {
    "id": "nOkHijttl10O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d9f31d59-592d-42c5-881e-50395683dd3d"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8533834586466166\n",
      "0.8535633484162897\n",
      "0.8615384615384616\n",
      "0.8455882352941176\n",
      "0.7069467841755195\n",
      "0.9384363163547969\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save('DPPIV_tensorflow_model',save_format = 'tf') \n",
    "!zip -r /content/DPPIV_tensorflow_model.zip /content/DPPIV_tensorflow_model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDNAw5DCUDHT",
    "outputId": "486b3c68-71c5-4c4b-f8bc-ae686a8c48fb"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: content/DPPIV_tensorflow_model/ (stored 0%)\n",
      "  adding: content/DPPIV_tensorflow_model/variables/ (stored 0%)\n",
      "  adding: content/DPPIV_tensorflow_model/variables/variables.index (deflated 64%)\n",
      "  adding: content/DPPIV_tensorflow_model/variables/variables.data-00000-of-00001 (deflated 41%)\n",
      "  adding: content/DPPIV_tensorflow_model/saved_model.pb (deflated 88%)\n",
      "  adding: content/DPPIV_tensorflow_model/assets/ (stored 0%)\n",
      "  adding: content/DPPIV_tensorflow_model/keras_metadata.pb (deflated 89%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.evaluate(X_test , y_test)"
   ],
   "metadata": {
    "id": "LB4VkASsiTG-",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "17087669-771f-440d-aa37-79b5551f26b1"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9/9 [==============================] - 0s 8ms/step - loss: 0.3511 - accuracy: 0.8534\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.35106101632118225, 0.853383481502533]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fURDBSCz6q6u"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
