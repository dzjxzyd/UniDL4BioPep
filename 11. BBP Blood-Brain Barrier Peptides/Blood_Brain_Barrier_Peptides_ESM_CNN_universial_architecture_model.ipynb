{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements for the following codings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fair-esm\n",
      "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 1.0 MB/s \n",
      "\u001b[?25hInstalling collected packages: fair-esm\n",
      "Successfully installed fair-esm-2.0.0\n"
     ]
    }
   ],
   "source": [
    "### packages required \n",
    "!pip install fair-esm \n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
    "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
    "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esm_embeddings(peptide_sequence_list):\n",
    "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
    "  #         or you have too many sequences for transformation in a single converting, \n",
    "  #         you conputer might automatically kill the job.\n",
    "  import torch\n",
    "  import esm\n",
    "  import collections\n",
    "  # load the model\n",
    "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
    "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  # load the peptide sequence list into the bach_converter\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "  ## batch tokens are the embedding results of the whole data set\n",
    "\n",
    "  # Extract per-residue representations (on CPU)\n",
    "  with torch.no_grad():\n",
    "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
    "  token_representations = results[\"representations\"][6]\n",
    "\n",
    "  # Generate per-sequence representations via averaging\n",
    "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "  sequence_representations = []\n",
    "  for i, tokens_len in enumerate(batch_lens):\n",
    "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
    "  # save dataset\n",
    "  # sequence_representations is a list and each element is a tensor\n",
    "  embeddings_results = collections.defaultdict(list)\n",
    "  for i in range(len(sequence_representations)):\n",
    "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "      each_seq_rep = sequence_representations[i].tolist()\n",
    "      for each_element in each_seq_rep:\n",
    "          embeddings_results[i].append(each_element)\n",
    "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "  return embeddings_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading and embeddings (main dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset loading\n",
    "dataset = pd.read_excel('BBP_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "\n",
    "embeddings_results = pd.DataFrame()\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "\n",
    "embeddings_results.to_csv('BBP_train_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "\n",
    "# loading the y dataset for model development \n",
    "y_train = dataset['label']\n",
    "y_train = np.array(y_train) # transformed as np.array for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset loading\n",
    "dataset = pd.read_excel('BBP_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "sequence_list = dataset['sequence'] \n",
    "embeddings_results = pd.DataFrame()\n",
    "# embedding all the peptide one by one\n",
    "for seq in sequence_list:\n",
    "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
    "    tuple_sequence = tuple(format_seq)\n",
    "    peptide_sequence_list = []\n",
    "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
    "    # employ ESM model for converting and save the converted data in csv format\n",
    "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
    "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
    "\n",
    "embeddings_results.to_csv('BBP_test_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
    "\n",
    "# loading the y dataset for model development \n",
    "y_test = dataset['label']\n",
    "y_test = np.array(y_test) # transformed as np.array for CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160, 320)\n",
      "(40, 320)\n",
      "(38, 320)\n",
      "(160,)\n",
      "(40,)\n",
      "(38,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['minmax_scaler.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training dataset loading\n",
    "dataset = pd.read_excel('BBP_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "# loading the y dataset for model development \n",
    "y_train = dataset['label']\n",
    "y_train = np.array(y_train) # transformed as np.array for CNN model\n",
    "# test dataset loading\n",
    "dataset = pd.read_excel('BBP_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
    "# loading the y dataset for model development \n",
    "y_test = dataset['label']\n",
    "y_test = np.array(y_test) # transformed as np.array for CNN model\n",
    "# assign the dataset \n",
    "X_train_data_name = 'BBP_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_test_data_name = 'BBP_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
    "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
    "\n",
    "X_train = np.array(X_train_data)\n",
    "X_test = np.array(X_test_data)\n",
    "\n",
    "\n",
    "# re-divide dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=123,shuffle=True, stratify=y_train)\n",
    "\n",
    "# normalize the X data range\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_sub)\n",
    "X_train_sub = scaler.transform(X_train_sub) # normalize X to 0-1 range\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# check the dimension of the dataset before model development\n",
    "print(X_train_sub.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train_sub.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Save the scaler to a file\n",
    "import joblib\n",
    "joblib.dump(scaler, 'minmax_scaler.pkl')\n",
    "# # Load the scaler from the file\n",
    "# loaded_scaler = joblib.load('minmax_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense, Activation, BatchNormalization, Flatten, Conv1D,Dropout, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "import keras\n",
    "from keras import backend as K\n",
    "def ESM_CNN(X_train, y_train, X_valid, y_valid):\n",
    "  inputShape=(320,1)\n",
    "  input = Input(inputShape)\n",
    "  x = Conv1D(32,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
    "  x = Dropout(0.15)(x)\n",
    "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
    "  model = Model(inputs = input,outputs = x,name='Predict')\n",
    "  # define SGD optimizer\n",
    "  momentum = 0.5\n",
    "  sgd = SGD(learning_rate=0.01, momentum=momentum, nesterov=False)\n",
    "  # compile the model\n",
    "  model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "  # learning deccay setting\n",
    "  import math\n",
    "  def step_decay(epoch): # gradually decrease the learning rate\n",
    "      initial_lrate=0.1\n",
    "      drop=0.6\n",
    "      epochs_drop = 3.0\n",
    "      lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
    "            math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
    "      return lrate\n",
    "  lrate = LearningRateScheduler(step_decay)\n",
    "  # early stop setting\n",
    "  early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,restore_best_weights = True)\n",
    "  mc = ModelCheckpoint('best_model.keras',  monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=False)\n",
    "  # summary the callbacks_list\n",
    "  callbacks_list = [ lrate , early_stop, mc]\n",
    "  model_history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                            epochs=200, callbacks=callbacks_list,batch_size = 8, verbose=2)\n",
    "  return model, model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import math\n",
    "def model_evaluation(model_name, X_test, y_test):\n",
    "    # Load the saved model\n",
    "    saved_model = load_model(model_name)\n",
    "    # Predict class probabilities\n",
    "    predicted_protability = saved_model.predict(X_test, batch_size=1) \n",
    "    predicted_class = np.argmax(predicted_protability, axis=1) # operating horizontally /// row-wise\n",
    "    # True labels\n",
    "    y_true = y_test\n",
    "    # Calculate confusion matrix components\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    # Calculate evaluation metrics\n",
    "    ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Sn = TP / (TP + FN)\n",
    "    Sp = TN / (TN + FP)\n",
    "    MCC = (TP * TN - FP * FN) / (math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))) \n",
    "    BACC = 0.5 * Sn + 0.5 * Sp\n",
    "    AUC = roc_auc_score(y_true, predicted_protability[:, 1])\n",
    "    # Print results\n",
    "    print(\"Accuracy:\", ACC)\n",
    "    print(\"Balanced Accuracy:\", BACC)\n",
    "    print(\"Sensitivity (Recall):\", Sn)\n",
    "    print(\"Specificity:\", Sp)\n",
    "    print(\"MCC:\", MCC)\n",
    "    print(\"AUC:\", AUC)\n",
    "    return ACC, BACC, Sn, Sp, MCC, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_history = ESM_CNN(X_train_sub, y_train_sub, X_val , y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 425us/step\n",
      "Accuracy: 0.868421052631579\n",
      "Balanced Accuracy: 0.868421052631579\n",
      "Sensitivity (Recall): 0.7894736842105263\n",
      "Specificity: 0.9473684210526315\n",
      "MCC: 0.7462025072446365\n",
      "AUC: 0.9279778393351801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.868421052631579,\n",
       " 0.868421052631579,\n",
       " 0.7894736842105263,\n",
       " 0.9473684210526315,\n",
       " 0.7462025072446365,\n",
       " 0.9279778393351801)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation('best_model.keras',X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_keras_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
