{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### requirements for the following codings\n"
      ],
      "metadata": {
        "id": "95NTckuFZZzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### packages required \n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "UO71IBS6ZgZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78a2e14d-e004-4c74-e5e9-f848acaaccd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.1.21)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.12.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=ff785a2e4dc4d7b63c0f44ba0f171e7b8bf2ceda8c8a2f1b5ab2d2cbd324c626\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
        "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
        "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
      ],
      "metadata": {
        "id": "m91cA0H5w_eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def esm_embeddings(peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you conputer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  # load the model\n",
        "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
        "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
        "  token_representations = results[\"representations\"][6]\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  return embeddings_results"
      ],
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data loading and embeddings"
      ],
      "metadata": {
        "id": "RddxugbsdR1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n6NOFoREw-40"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('BBP_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('BBP_train_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_train = dataset['label']\n",
        "y_train = np.array(y_train) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "LNlD8pvizH84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fca6b1-f612-4872-d620-d138d90b5ba2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D.pt\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to /root/.cache/torch/hub/checkpoints/esm2_t6_8M_UR50D-contact-regression.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset loading\n",
        "dataset = pd.read_excel('BBP_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('BBP_test_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_test = dataset['label']\n",
        "y_test = np.array(y_test) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "U7jxoIsCw8dW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the dataset \n",
        "X_train_data_name = 'BBP_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_test_data_name = 'BBP_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_train = np.array(X_train_data)\n",
        "X_test = np.array(X_test_data)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Xk13-JbBXAph"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the dimension of the dataset before model development\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HubTATKXslKw",
        "outputId": "2dff7465-ab78-433d-f5b5-814a92fc0c01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 320)\n",
            "(38, 320)\n",
            "(200,)\n",
            "(38,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architecture"
      ],
      "metadata": {
        "id": "U3Fagh9Iw83q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ESM_CNN(X_train, y_train, X_test, y_test):\n",
        "  from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D,Conv1D\n",
        "  from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, AveragePooling1D, MaxPooling1D\n",
        "  from keras.models import Sequential,Model\n",
        "  from keras.optimizers import SGD\n",
        "  from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "  import keras\n",
        "  from keras import backend as K\n",
        "  inputShape=(320,1)\n",
        "  input = Input(inputShape)\n",
        "  x = Conv1D(128,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Conv1D(32,(3),strides = (1),name='layer_conv2',padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "  model = Model(inputs = input,outputs = x,name='Predict')\n",
        "  # define SGD optimizer\n",
        "  momentum = 0.5\n",
        "  sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "  # learning deccay setting\n",
        "  import math\n",
        "  def step_decay(epoch): # gradually decrease the learning rate \n",
        "      initial_lrate=0.1\n",
        "      drop=0.6\n",
        "      epochs_drop = 3.0\n",
        "      lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "            math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "      return lrate\n",
        "  lrate = LearningRateScheduler(step_decay)\n",
        "\n",
        "  # early stop setting\n",
        "  early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,restore_best_weights = True)\n",
        "\n",
        "  # summary the callbacks_list\n",
        "  callbacks_list = [ lrate , early_stop]\n",
        "\n",
        "  model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                            epochs=200,callbacks=callbacks_list,batch_size = 8, verbose=1)\n",
        "  return model, model_history"
      ],
      "metadata": {
        "id": "b0QeK6-Cg_cv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10-fold cross validation"
      ],
      "metadata": {
        "id": "sws_G8h08tuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing 10-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "k = 10 \n",
        "kf = KFold(n_splits=k, shuffle = True, random_state=1)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "\n",
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "\n",
        "for train_index , test_index in kf.split(y_train):\n",
        "    X_train_CV , X_valid_CV = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
        "    y_train_CV , y_valid_CV = y_train.iloc[train_index] , y_train.iloc[test_index]\n",
        "    model, model_history = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = model.predict(X_valid_CV,batch_size=1)\n",
        "    for i in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_valid_CV    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_valid_CV, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n"
      ],
      "metadata": {
        "id": "iFGZ88goj6u4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bd026e-81f4-4b8b-b7c5-a6bd746d4d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 13ms/step - loss: 1.6098 - accuracy: 0.8790 - val_loss: 0.3579 - val_accuracy: 0.8333 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 15ms/step - loss: 0.1665 - accuracy: 0.9455 - val_loss: 0.3097 - val_accuracy: 0.8636 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 13ms/step - loss: 0.0987 - accuracy: 0.9642 - val_loss: 0.1854 - val_accuracy: 0.9091 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0801 - accuracy: 0.9710 - val_loss: 0.1928 - val_accuracy: 0.9394 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0707 - accuracy: 0.9779 - val_loss: 0.2465 - val_accuracy: 0.9091 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0473 - accuracy: 0.9847 - val_loss: 0.2469 - val_accuracy: 0.9091 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0449 - accuracy: 0.9830 - val_loss: 0.2481 - val_accuracy: 0.9091 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.3024 - val_accuracy: 0.9242 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0333 - accuracy: 0.9898 - val_loss: 0.3185 - val_accuracy: 0.9091 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.3058 - val_accuracy: 0.9242 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.3295 - val_accuracy: 0.9242 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0198 - accuracy: 0.9932 - val_loss: 0.3323 - val_accuracy: 0.9242 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0194 - accuracy: 0.9949 - val_loss: 0.3275 - val_accuracy: 0.9242 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.3336 - val_accuracy: 0.9091 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.3384 - val_accuracy: 0.9242 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.3429 - val_accuracy: 0.9242 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.3457 - val_accuracy: 0.9242 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0149 - accuracy: 0.9932 - val_loss: 0.3434 - val_accuracy: 0.9242 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0154 - accuracy: 0.9966 - val_loss: 0.3464 - val_accuracy: 0.9242 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 0.3491 - val_accuracy: 0.9091 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 0.3486 - val_accuracy: 0.9091 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0127 - accuracy: 0.9966 - val_loss: 0.3463 - val_accuracy: 0.9091 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0124 - accuracy: 0.9949 - val_loss: 0.3472 - val_accuracy: 0.9091 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.3480 - val_accuracy: 0.9091 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 0.3482 - val_accuracy: 0.9091 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.3494 - val_accuracy: 0.9091 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.3514 - val_accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9983 - val_loss: 0.3527 - val_accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.3518 - val_accuracy: 0.9091 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0180 - accuracy: 0.9932 - val_loss: 0.3502 - val_accuracy: 0.9091 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.3506 - val_accuracy: 0.9091 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0158 - accuracy: 0.9949 - val_loss: 0.3509 - val_accuracy: 0.9091 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.3508 - val_accuracy: 0.9091 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0129 - accuracy: 0.9983 - val_loss: 0.3502 - val_accuracy: 0.9091 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0123 - accuracy: 0.9932 - val_loss: 0.3503 - val_accuracy: 0.9091 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9091 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.3513 - val_accuracy: 0.9091 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.3519 - val_accuracy: 0.9091 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0085 - accuracy: 0.9983 - val_loss: 0.3528 - val_accuracy: 0.9091 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 0.3528 - val_accuracy: 0.9091 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.3536 - val_accuracy: 0.9091 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0187 - accuracy: 0.9915 - val_loss: 0.3529 - val_accuracy: 0.9091 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0168 - accuracy: 0.9932 - val_loss: 0.3521 - val_accuracy: 0.9091 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0172 - accuracy: 0.9915 - val_loss: 0.3531 - val_accuracy: 0.9091 - lr: 7.8364e-05\n",
            "66/66 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 13ms/step - loss: 0.8670 - accuracy: 0.9012 - val_loss: 0.5982 - val_accuracy: 0.7727 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.1289 - accuracy: 0.9455 - val_loss: 0.6094 - val_accuracy: 0.7727 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0869 - accuracy: 0.9710 - val_loss: 0.5888 - val_accuracy: 0.7727 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0718 - accuracy: 0.9693 - val_loss: 0.5411 - val_accuracy: 0.7879 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0595 - accuracy: 0.9847 - val_loss: 0.3931 - val_accuracy: 0.8939 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0292 - accuracy: 0.9881 - val_loss: 0.2524 - val_accuracy: 0.9091 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.2570 - val_accuracy: 0.9242 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 0.3218 - val_accuracy: 0.9091 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0204 - accuracy: 0.9915 - val_loss: 0.3303 - val_accuracy: 0.9394 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 12ms/step - loss: 0.0153 - accuracy: 0.9932 - val_loss: 0.3931 - val_accuracy: 0.9242 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 17ms/step - loss: 0.0129 - accuracy: 0.9949 - val_loss: 0.4037 - val_accuracy: 0.9242 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 16ms/step - loss: 0.0136 - accuracy: 0.9932 - val_loss: 0.4152 - val_accuracy: 0.9394 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 16ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.4489 - val_accuracy: 0.9394 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 17ms/step - loss: 0.0122 - accuracy: 0.9932 - val_loss: 0.4143 - val_accuracy: 0.9242 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.4060 - val_accuracy: 0.9242 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.4357 - val_accuracy: 0.9394 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0101 - accuracy: 0.9949 - val_loss: 0.4602 - val_accuracy: 0.9242 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.4654 - val_accuracy: 0.9242 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.4548 - val_accuracy: 0.9394 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 0.4568 - val_accuracy: 0.9394 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.4604 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0132 - accuracy: 0.9932 - val_loss: 0.4748 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.4759 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0083 - accuracy: 0.9966 - val_loss: 0.4687 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.4735 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0068 - accuracy: 0.9983 - val_loss: 0.4758 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.4766 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0104 - accuracy: 0.9949 - val_loss: 0.4779 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0071 - accuracy: 0.9966 - val_loss: 0.4780 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0100 - accuracy: 0.9949 - val_loss: 0.4796 - val_accuracy: 0.9394 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0098 - accuracy: 0.9966 - val_loss: 0.4788 - val_accuracy: 0.9394 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9915 - val_loss: 0.4774 - val_accuracy: 0.9394 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.4782 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0095 - accuracy: 0.9949 - val_loss: 0.4798 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0102 - accuracy: 0.9949 - val_loss: 0.4778 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.4776 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.4778 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 0.4779 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0075 - accuracy: 0.9983 - val_loss: 0.4777 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.4781 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.4784 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.4778 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.4776 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.4774 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.4773 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.4776 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 47/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.4767 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 48/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.4781 - val_accuracy: 0.9394 - lr: 2.8211e-05\n",
            "Epoch 49/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.4782 - val_accuracy: 0.9394 - lr: 2.8211e-05\n",
            "66/66 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 0.8826 - accuracy: 0.8228 - val_loss: 0.4593 - val_accuracy: 0.9242 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.2772 - accuracy: 0.8825 - val_loss: 0.7313 - val_accuracy: 0.3485 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1438 - accuracy: 0.9455 - val_loss: 0.5598 - val_accuracy: 0.7727 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1247 - accuracy: 0.9540 - val_loss: 0.7455 - val_accuracy: 0.4848 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1132 - accuracy: 0.9574 - val_loss: 0.2589 - val_accuracy: 0.9394 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0859 - accuracy: 0.9727 - val_loss: 0.1935 - val_accuracy: 0.9545 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0721 - accuracy: 0.9744 - val_loss: 0.2034 - val_accuracy: 0.9545 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 0.2330 - val_accuracy: 0.9697 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0588 - accuracy: 0.9830 - val_loss: 0.2454 - val_accuracy: 0.9697 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0405 - accuracy: 0.9881 - val_loss: 0.2603 - val_accuracy: 0.9545 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0316 - accuracy: 0.9915 - val_loss: 0.2889 - val_accuracy: 0.9545 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0280 - accuracy: 0.9932 - val_loss: 0.3068 - val_accuracy: 0.9545 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0367 - accuracy: 0.9830 - val_loss: 0.3301 - val_accuracy: 0.9545 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0302 - accuracy: 0.9932 - val_loss: 0.3273 - val_accuracy: 0.9545 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0338 - accuracy: 0.9864 - val_loss: 0.3017 - val_accuracy: 0.9545 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0305 - accuracy: 0.9881 - val_loss: 0.3048 - val_accuracy: 0.9545 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0308 - accuracy: 0.9915 - val_loss: 0.3092 - val_accuracy: 0.9697 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0303 - accuracy: 0.9932 - val_loss: 0.3181 - val_accuracy: 0.9545 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0355 - accuracy: 0.9830 - val_loss: 0.3235 - val_accuracy: 0.9545 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0294 - accuracy: 0.9932 - val_loss: 0.3299 - val_accuracy: 0.9394 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0327 - accuracy: 0.9881 - val_loss: 0.3278 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0242 - accuracy: 0.9949 - val_loss: 0.3259 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0194 - accuracy: 0.9949 - val_loss: 0.3345 - val_accuracy: 0.9394 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0278 - accuracy: 0.9881 - val_loss: 0.3408 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0348 - accuracy: 0.9864 - val_loss: 0.3470 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0206 - accuracy: 0.9966 - val_loss: 0.3438 - val_accuracy: 0.9394 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0291 - accuracy: 0.9915 - val_loss: 0.3467 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.3466 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0214 - accuracy: 0.9949 - val_loss: 0.3493 - val_accuracy: 0.9394 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0201 - accuracy: 0.9966 - val_loss: 0.3500 - val_accuracy: 0.9394 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0207 - accuracy: 0.9949 - val_loss: 0.3522 - val_accuracy: 0.9545 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0273 - accuracy: 0.9881 - val_loss: 0.3514 - val_accuracy: 0.9394 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0358 - accuracy: 0.9881 - val_loss: 0.3523 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0333 - accuracy: 0.9881 - val_loss: 0.3528 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.3529 - val_accuracy: 0.9394 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0257 - accuracy: 0.9898 - val_loss: 0.3533 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0325 - accuracy: 0.9864 - val_loss: 0.3515 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0278 - accuracy: 0.9881 - val_loss: 0.3522 - val_accuracy: 0.9394 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0241 - accuracy: 0.9915 - val_loss: 0.3520 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0284 - accuracy: 0.9932 - val_loss: 0.3516 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0309 - accuracy: 0.9881 - val_loss: 0.3530 - val_accuracy: 0.9394 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0255 - accuracy: 0.9898 - val_loss: 0.3531 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0318 - accuracy: 0.9881 - val_loss: 0.3535 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0248 - accuracy: 0.9915 - val_loss: 0.3538 - val_accuracy: 0.9394 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0263 - accuracy: 0.9898 - val_loss: 0.3535 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0203 - accuracy: 0.9949 - val_loss: 0.3535 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 47/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 0.3531 - val_accuracy: 0.9394 - lr: 4.7018e-05\n",
            "Epoch 48/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0231 - accuracy: 0.9915 - val_loss: 0.3534 - val_accuracy: 0.9394 - lr: 2.8211e-05\n",
            "66/66 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 1.1171 - accuracy: 0.8724 - val_loss: 0.4136 - val_accuracy: 0.8923 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1548 - accuracy: 0.9320 - val_loss: 0.4567 - val_accuracy: 0.9231 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0885 - accuracy: 0.9609 - val_loss: 0.3821 - val_accuracy: 0.9538 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0581 - accuracy: 0.9779 - val_loss: 0.1890 - val_accuracy: 0.9692 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0624 - accuracy: 0.9796 - val_loss: 0.1410 - val_accuracy: 0.9692 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0354 - accuracy: 0.9864 - val_loss: 0.1113 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0163 - accuracy: 0.9966 - val_loss: 0.1270 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0213 - accuracy: 0.9949 - val_loss: 0.1546 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0169 - accuracy: 0.9966 - val_loss: 0.1757 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0199 - accuracy: 0.9915 - val_loss: 0.1769 - val_accuracy: 0.9538 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 0.2091 - val_accuracy: 0.9538 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.2127 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.2175 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0164 - accuracy: 0.9983 - val_loss: 0.1948 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 8ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.2009 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.2126 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0111 - accuracy: 0.9983 - val_loss: 0.2197 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2235 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0114 - accuracy: 0.9949 - val_loss: 0.2210 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.2282 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2281 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.2271 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.2290 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2287 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2279 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.2271 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2284 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2283 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2291 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0089 - accuracy: 0.9983 - val_loss: 0.2293 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.2288 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2286 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0092 - accuracy: 0.9966 - val_loss: 0.2285 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2288 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2291 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0098 - accuracy: 0.9949 - val_loss: 0.2294 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.2289 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.2286 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2291 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2295 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0097 - accuracy: 0.9983 - val_loss: 0.2294 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "65/65 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 0.9284 - accuracy: 0.8690 - val_loss: 0.4879 - val_accuracy: 0.8769 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.1600 - accuracy: 0.9490 - val_loss: 0.4814 - val_accuracy: 0.8769 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1005 - accuracy: 0.9609 - val_loss: 0.4180 - val_accuracy: 0.8769 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0711 - accuracy: 0.9796 - val_loss: 0.2955 - val_accuracy: 0.9231 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0590 - accuracy: 0.9847 - val_loss: 0.1618 - val_accuracy: 0.9692 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0594 - accuracy: 0.9762 - val_loss: 0.0894 - val_accuracy: 0.9846 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0370 - accuracy: 0.9864 - val_loss: 0.0380 - val_accuracy: 0.9846 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0402 - accuracy: 0.9864 - val_loss: 0.0193 - val_accuracy: 0.9846 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0363 - accuracy: 0.9847 - val_loss: 0.0314 - val_accuracy: 0.9846 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0236 - accuracy: 0.9915 - val_loss: 0.0373 - val_accuracy: 0.9846 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0177 - accuracy: 0.9949 - val_loss: 0.0490 - val_accuracy: 0.9846 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 0.0503 - val_accuracy: 0.9846 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0191 - accuracy: 0.9932 - val_loss: 0.0457 - val_accuracy: 0.9846 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0159 - accuracy: 0.9932 - val_loss: 0.0436 - val_accuracy: 0.9846 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0154 - accuracy: 0.9932 - val_loss: 0.0461 - val_accuracy: 0.9846 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0143 - accuracy: 0.9932 - val_loss: 0.0464 - val_accuracy: 0.9846 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0157 - accuracy: 0.9915 - val_loss: 0.0435 - val_accuracy: 0.9846 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0191 - accuracy: 0.9898 - val_loss: 0.0418 - val_accuracy: 0.9846 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.0455 - val_accuracy: 0.9846 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.0487 - val_accuracy: 0.9846 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 0.0472 - val_accuracy: 0.9846 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0095 - accuracy: 0.9983 - val_loss: 0.0487 - val_accuracy: 0.9846 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.0460 - val_accuracy: 0.9846 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9966 - val_loss: 0.0462 - val_accuracy: 0.9846 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 0.0441 - val_accuracy: 0.9846 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0133 - accuracy: 0.9932 - val_loss: 0.0414 - val_accuracy: 0.9846 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0440 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0125 - accuracy: 0.9949 - val_loss: 0.0421 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 0.0429 - val_accuracy: 0.9846 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0433 - val_accuracy: 0.9846 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0435 - val_accuracy: 0.9846 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.0439 - val_accuracy: 0.9846 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0436 - val_accuracy: 0.9846 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0108 - accuracy: 0.9949 - val_loss: 0.0439 - val_accuracy: 0.9846 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 0.0440 - val_accuracy: 0.9846 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 0.0439 - val_accuracy: 0.9846 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0122 - accuracy: 0.9949 - val_loss: 0.0440 - val_accuracy: 0.9846 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0139 - accuracy: 0.9932 - val_loss: 0.0435 - val_accuracy: 0.9846 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0124 - accuracy: 0.9949 - val_loss: 0.0436 - val_accuracy: 0.9846 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0118 - accuracy: 0.9966 - val_loss: 0.0436 - val_accuracy: 0.9846 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 0.0436 - val_accuracy: 0.9846 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0112 - accuracy: 0.9949 - val_loss: 0.0438 - val_accuracy: 0.9846 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0179 - accuracy: 0.9915 - val_loss: 0.0438 - val_accuracy: 0.9846 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 0.0438 - val_accuracy: 0.9846 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0094 - accuracy: 0.9949 - val_loss: 0.0439 - val_accuracy: 0.9846 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0121 - accuracy: 0.9949 - val_loss: 0.0440 - val_accuracy: 0.9846 - lr: 4.7018e-05\n",
            "65/65 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 0.6295 - accuracy: 0.8401 - val_loss: 0.5943 - val_accuracy: 0.8308 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.2189 - accuracy: 0.9150 - val_loss: 0.5965 - val_accuracy: 0.8308 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.1414 - accuracy: 0.9490 - val_loss: 0.6256 - val_accuracy: 0.8308 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1172 - accuracy: 0.9558 - val_loss: 0.5949 - val_accuracy: 0.8462 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0955 - accuracy: 0.9609 - val_loss: 0.4633 - val_accuracy: 0.9077 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0926 - accuracy: 0.9677 - val_loss: 0.1674 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0654 - accuracy: 0.9660 - val_loss: 0.0904 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0668 - accuracy: 0.9762 - val_loss: 0.1340 - val_accuracy: 0.9538 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0510 - accuracy: 0.9864 - val_loss: 0.0868 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0539 - accuracy: 0.9796 - val_loss: 0.0795 - val_accuracy: 0.9538 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 0.1033 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0323 - accuracy: 0.9881 - val_loss: 0.1042 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0295 - accuracy: 0.9915 - val_loss: 0.1157 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0318 - accuracy: 0.9847 - val_loss: 0.1228 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 0.1146 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0288 - accuracy: 0.9881 - val_loss: 0.1152 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0250 - accuracy: 0.9915 - val_loss: 0.1285 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0195 - accuracy: 0.9898 - val_loss: 0.1341 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0274 - accuracy: 0.9898 - val_loss: 0.1260 - val_accuracy: 0.9692 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0233 - accuracy: 0.9881 - val_loss: 0.1259 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0292 - accuracy: 0.9915 - val_loss: 0.1259 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0259 - accuracy: 0.9932 - val_loss: 0.1244 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0277 - accuracy: 0.9881 - val_loss: 0.1274 - val_accuracy: 0.9538 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.1297 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 0.1284 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: 0.1254 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0228 - accuracy: 0.9915 - val_loss: 0.1257 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.1281 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9949 - val_loss: 0.1278 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0202 - accuracy: 0.9949 - val_loss: 0.1287 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0227 - accuracy: 0.9915 - val_loss: 0.1294 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0223 - accuracy: 0.9898 - val_loss: 0.1285 - val_accuracy: 0.9538 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0216 - accuracy: 0.9915 - val_loss: 0.1286 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0221 - accuracy: 0.9915 - val_loss: 0.1287 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0157 - accuracy: 0.9983 - val_loss: 0.1291 - val_accuracy: 0.9538 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0243 - accuracy: 0.9881 - val_loss: 0.1294 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0198 - accuracy: 0.9949 - val_loss: 0.1294 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.1296 - val_accuracy: 0.9538 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 0.1296 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.1299 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0160 - accuracy: 0.9949 - val_loss: 0.1299 - val_accuracy: 0.9538 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0225 - accuracy: 0.9898 - val_loss: 0.1297 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0173 - accuracy: 0.9932 - val_loss: 0.1298 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0208 - accuracy: 0.9915 - val_loss: 0.1297 - val_accuracy: 0.9538 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0212 - accuracy: 0.9915 - val_loss: 0.1297 - val_accuracy: 0.9538 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0243 - accuracy: 0.9881 - val_loss: 0.1298 - val_accuracy: 0.9538 - lr: 4.7018e-05\n",
            "65/65 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 0.8537 - accuracy: 0.8673 - val_loss: 0.5954 - val_accuracy: 0.7385 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1542 - accuracy: 0.9371 - val_loss: 0.5554 - val_accuracy: 0.7385 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1201 - accuracy: 0.9439 - val_loss: 0.5103 - val_accuracy: 0.8308 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0837 - accuracy: 0.9626 - val_loss: 0.4487 - val_accuracy: 0.8769 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0787 - accuracy: 0.9660 - val_loss: 0.3000 - val_accuracy: 0.9077 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0625 - accuracy: 0.9762 - val_loss: 0.1736 - val_accuracy: 0.9231 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 0.1359 - val_accuracy: 0.9385 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0536 - accuracy: 0.9796 - val_loss: 0.1258 - val_accuracy: 0.9538 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0371 - accuracy: 0.9847 - val_loss: 0.1152 - val_accuracy: 0.9385 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0342 - accuracy: 0.9864 - val_loss: 0.1251 - val_accuracy: 0.9385 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0390 - accuracy: 0.9847 - val_loss: 0.1140 - val_accuracy: 0.9538 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 0.1225 - val_accuracy: 0.9692 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0278 - accuracy: 0.9898 - val_loss: 0.1168 - val_accuracy: 0.9538 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.1328 - val_accuracy: 0.9692 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0177 - accuracy: 0.9932 - val_loss: 0.1477 - val_accuracy: 0.9692 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0192 - accuracy: 0.9915 - val_loss: 0.1354 - val_accuracy: 0.9385 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0202 - accuracy: 0.9898 - val_loss: 0.1269 - val_accuracy: 0.9538 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 11ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.1302 - val_accuracy: 0.9385 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0268 - accuracy: 0.9898 - val_loss: 0.1295 - val_accuracy: 0.9538 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 0.1290 - val_accuracy: 0.9385 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0151 - accuracy: 0.9966 - val_loss: 0.1316 - val_accuracy: 0.9385 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0207 - accuracy: 0.9898 - val_loss: 0.1282 - val_accuracy: 0.9385 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0151 - accuracy: 0.9983 - val_loss: 0.1316 - val_accuracy: 0.9385 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.1319 - val_accuracy: 0.9385 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0167 - accuracy: 0.9898 - val_loss: 0.1338 - val_accuracy: 0.9385 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9949 - val_loss: 0.1383 - val_accuracy: 0.9538 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0201 - accuracy: 0.9881 - val_loss: 0.1367 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0177 - accuracy: 0.9932 - val_loss: 0.1346 - val_accuracy: 0.9538 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0121 - accuracy: 0.9983 - val_loss: 0.1340 - val_accuracy: 0.9385 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 0.1327 - val_accuracy: 0.9385 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0157 - accuracy: 0.9932 - val_loss: 0.1326 - val_accuracy: 0.9385 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0133 - accuracy: 0.9949 - val_loss: 0.1321 - val_accuracy: 0.9385 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0155 - accuracy: 0.9949 - val_loss: 0.1325 - val_accuracy: 0.9385 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0106 - accuracy: 0.9983 - val_loss: 0.1326 - val_accuracy: 0.9385 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0117 - accuracy: 0.9983 - val_loss: 0.1326 - val_accuracy: 0.9385 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0110 - accuracy: 0.9949 - val_loss: 0.1330 - val_accuracy: 0.9385 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0151 - accuracy: 0.9949 - val_loss: 0.1333 - val_accuracy: 0.9385 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 15ms/step - loss: 0.0133 - accuracy: 0.9966 - val_loss: 0.1334 - val_accuracy: 0.9385 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 18ms/step - loss: 0.0200 - accuracy: 0.9898 - val_loss: 0.1333 - val_accuracy: 0.9385 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 17ms/step - loss: 0.0168 - accuracy: 0.9915 - val_loss: 0.1334 - val_accuracy: 0.9385 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 18ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 14ms/step - loss: 0.0142 - accuracy: 0.9949 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 0.1338 - val_accuracy: 0.9385 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.1336 - val_accuracy: 0.9385 - lr: 4.7018e-05\n",
            "Epoch 47/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0141 - accuracy: 0.9932 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 4.7018e-05\n",
            "Epoch 48/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 2.8211e-05\n",
            "Epoch 49/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 0.1336 - val_accuracy: 0.9385 - lr: 2.8211e-05\n",
            "Epoch 50/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0148 - accuracy: 0.9915 - val_loss: 0.1337 - val_accuracy: 0.9385 - lr: 2.8211e-05\n",
            "Epoch 51/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0113 - accuracy: 0.9949 - val_loss: 0.1336 - val_accuracy: 0.9385 - lr: 1.6927e-05\n",
            "Epoch 52/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0133 - accuracy: 0.9932 - val_loss: 0.1336 - val_accuracy: 0.9385 - lr: 1.6927e-05\n",
            "65/65 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 13ms/step - loss: 1.6665 - accuracy: 0.8605 - val_loss: 0.4005 - val_accuracy: 0.8462 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.1983 - accuracy: 0.9371 - val_loss: 0.3258 - val_accuracy: 0.8462 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.1124 - accuracy: 0.9626 - val_loss: 0.2616 - val_accuracy: 0.8769 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0984 - accuracy: 0.9711 - val_loss: 0.1681 - val_accuracy: 0.9231 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0894 - accuracy: 0.9677 - val_loss: 0.1390 - val_accuracy: 0.9385 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0600 - accuracy: 0.9728 - val_loss: 0.1150 - val_accuracy: 0.9538 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0562 - accuracy: 0.9762 - val_loss: 0.1085 - val_accuracy: 0.9385 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0604 - accuracy: 0.9830 - val_loss: 0.1202 - val_accuracy: 0.9231 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0393 - accuracy: 0.9898 - val_loss: 0.2102 - val_accuracy: 0.9231 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0353 - accuracy: 0.9881 - val_loss: 0.1386 - val_accuracy: 0.9385 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0352 - accuracy: 0.9830 - val_loss: 0.1623 - val_accuracy: 0.9231 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0272 - accuracy: 0.9881 - val_loss: 0.1873 - val_accuracy: 0.9231 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0309 - accuracy: 0.9881 - val_loss: 0.1639 - val_accuracy: 0.9231 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0227 - accuracy: 0.9915 - val_loss: 0.1688 - val_accuracy: 0.9385 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9881 - val_loss: 0.2264 - val_accuracy: 0.9231 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0286 - accuracy: 0.9864 - val_loss: 0.1806 - val_accuracy: 0.9385 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0299 - accuracy: 0.9898 - val_loss: 0.2076 - val_accuracy: 0.9231 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.2165 - val_accuracy: 0.9231 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0226 - accuracy: 0.9881 - val_loss: 0.2080 - val_accuracy: 0.9231 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0250 - accuracy: 0.9898 - val_loss: 0.2195 - val_accuracy: 0.9231 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.2085 - val_accuracy: 0.9231 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0219 - accuracy: 0.9898 - val_loss: 0.2054 - val_accuracy: 0.9231 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 0.2172 - val_accuracy: 0.9231 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0249 - accuracy: 0.9915 - val_loss: 0.2007 - val_accuracy: 0.9231 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0198 - accuracy: 0.9949 - val_loss: 0.2111 - val_accuracy: 0.9231 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0177 - accuracy: 0.9915 - val_loss: 0.2072 - val_accuracy: 0.9231 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0191 - accuracy: 0.9915 - val_loss: 0.2040 - val_accuracy: 0.9231 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0223 - accuracy: 0.9932 - val_loss: 0.2004 - val_accuracy: 0.9231 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0191 - accuracy: 0.9932 - val_loss: 0.1955 - val_accuracy: 0.9385 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0178 - accuracy: 0.9949 - val_loss: 0.1979 - val_accuracy: 0.9385 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0258 - accuracy: 0.9898 - val_loss: 0.2045 - val_accuracy: 0.9231 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0207 - accuracy: 0.9898 - val_loss: 0.2019 - val_accuracy: 0.9231 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0220 - accuracy: 0.9915 - val_loss: 0.2008 - val_accuracy: 0.9231 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0204 - accuracy: 0.9932 - val_loss: 0.2007 - val_accuracy: 0.9231 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0203 - accuracy: 0.9915 - val_loss: 0.2000 - val_accuracy: 0.9231 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0294 - accuracy: 0.9847 - val_loss: 0.1990 - val_accuracy: 0.9231 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0217 - accuracy: 0.9898 - val_loss: 0.2003 - val_accuracy: 0.9231 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0192 - accuracy: 0.9949 - val_loss: 0.2018 - val_accuracy: 0.9231 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0230 - accuracy: 0.9915 - val_loss: 0.2006 - val_accuracy: 0.9231 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 0.2018 - val_accuracy: 0.9231 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.2004 - val_accuracy: 0.9385 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.2009 - val_accuracy: 0.9385 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0277 - accuracy: 0.9864 - val_loss: 0.2008 - val_accuracy: 0.9385 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0202 - accuracy: 0.9915 - val_loss: 0.2011 - val_accuracy: 0.9231 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0206 - accuracy: 0.9915 - val_loss: 0.2012 - val_accuracy: 0.9231 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0226 - accuracy: 0.9915 - val_loss: 0.2012 - val_accuracy: 0.9231 - lr: 4.7018e-05\n",
            "65/65 [==============================] - 0s 2ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 13ms/step - loss: 1.1751 - accuracy: 0.8588 - val_loss: 0.3532 - val_accuracy: 0.8308 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1922 - accuracy: 0.9116 - val_loss: 0.3213 - val_accuracy: 0.9077 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.1374 - accuracy: 0.9456 - val_loss: 0.2222 - val_accuracy: 0.9538 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1081 - accuracy: 0.9507 - val_loss: 0.1145 - val_accuracy: 0.9846 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0951 - accuracy: 0.9524 - val_loss: 0.0369 - val_accuracy: 1.0000 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0673 - accuracy: 0.9694 - val_loss: 0.0091 - val_accuracy: 1.0000 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0714 - accuracy: 0.9728 - val_loss: 0.0095 - val_accuracy: 1.0000 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0522 - accuracy: 0.9830 - val_loss: 0.0059 - val_accuracy: 1.0000 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0466 - accuracy: 0.9864 - val_loss: 0.0041 - val_accuracy: 1.0000 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0323 - accuracy: 0.9864 - val_loss: 9.5903e-04 - val_accuracy: 1.0000 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0321 - accuracy: 0.9830 - val_loss: 8.4964e-04 - val_accuracy: 1.0000 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0352 - accuracy: 0.9881 - val_loss: 9.9756e-04 - val_accuracy: 1.0000 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0209 - accuracy: 0.9915 - val_loss: 6.8763e-04 - val_accuracy: 1.0000 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0257 - accuracy: 0.9898 - val_loss: 7.4097e-04 - val_accuracy: 1.0000 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0273 - accuracy: 0.9847 - val_loss: 7.4458e-04 - val_accuracy: 1.0000 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0193 - accuracy: 0.9966 - val_loss: 4.9089e-04 - val_accuracy: 1.0000 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0182 - accuracy: 0.9915 - val_loss: 5.1621e-04 - val_accuracy: 1.0000 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0213 - accuracy: 0.9932 - val_loss: 4.8318e-04 - val_accuracy: 1.0000 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0246 - accuracy: 0.9915 - val_loss: 4.7815e-04 - val_accuracy: 1.0000 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0241 - accuracy: 0.9932 - val_loss: 5.2546e-04 - val_accuracy: 1.0000 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0154 - accuracy: 0.9966 - val_loss: 5.1536e-04 - val_accuracy: 1.0000 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0143 - accuracy: 0.9932 - val_loss: 4.8367e-04 - val_accuracy: 1.0000 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0242 - accuracy: 0.9915 - val_loss: 5.4302e-04 - val_accuracy: 1.0000 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0230 - accuracy: 0.9949 - val_loss: 5.5716e-04 - val_accuracy: 1.0000 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0242 - accuracy: 0.9898 - val_loss: 5.4588e-04 - val_accuracy: 1.0000 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0166 - accuracy: 0.9949 - val_loss: 5.1323e-04 - val_accuracy: 1.0000 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 4.9576e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0235 - accuracy: 0.9915 - val_loss: 4.9531e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9983 - val_loss: 4.8734e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0203 - accuracy: 0.9966 - val_loss: 4.8499e-04 - val_accuracy: 1.0000 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 4.8068e-04 - val_accuracy: 1.0000 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0135 - accuracy: 0.9949 - val_loss: 4.8785e-04 - val_accuracy: 1.0000 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0237 - accuracy: 0.9932 - val_loss: 5.0384e-04 - val_accuracy: 1.0000 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0228 - accuracy: 0.9915 - val_loss: 5.0283e-04 - val_accuracy: 1.0000 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 5.0965e-04 - val_accuracy: 1.0000 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0193 - accuracy: 0.9949 - val_loss: 5.1066e-04 - val_accuracy: 1.0000 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0163 - accuracy: 0.9983 - val_loss: 5.1822e-04 - val_accuracy: 1.0000 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0176 - accuracy: 0.9915 - val_loss: 5.0950e-04 - val_accuracy: 1.0000 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0223 - accuracy: 0.9898 - val_loss: 5.0428e-04 - val_accuracy: 1.0000 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0176 - accuracy: 0.9932 - val_loss: 4.9982e-04 - val_accuracy: 1.0000 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0176 - accuracy: 0.9915 - val_loss: 5.0915e-04 - val_accuracy: 1.0000 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 5.1093e-04 - val_accuracy: 1.0000 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0250 - accuracy: 0.9898 - val_loss: 5.0924e-04 - val_accuracy: 1.0000 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0183 - accuracy: 0.9915 - val_loss: 5.0694e-04 - val_accuracy: 1.0000 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0179 - accuracy: 0.9949 - val_loss: 4.9869e-04 - val_accuracy: 1.0000 - lr: 4.7018e-05\n",
            "65/65 [==============================] - 0s 1ms/step\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 2s 12ms/step - loss: 1.1164 - accuracy: 0.8759 - val_loss: 0.3313 - val_accuracy: 0.8615 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.1361 - accuracy: 0.9524 - val_loss: 0.3515 - val_accuracy: 0.9077 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0902 - accuracy: 0.9609 - val_loss: 0.2833 - val_accuracy: 0.9385 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0720 - accuracy: 0.9677 - val_loss: 0.1863 - val_accuracy: 0.9231 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0534 - accuracy: 0.9762 - val_loss: 0.1458 - val_accuracy: 0.9538 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0448 - accuracy: 0.9830 - val_loss: 0.1299 - val_accuracy: 0.9538 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0328 - accuracy: 0.9847 - val_loss: 0.1461 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0302 - accuracy: 0.9881 - val_loss: 0.1723 - val_accuracy: 0.9692 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0197 - accuracy: 0.9898 - val_loss: 0.1851 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.1634 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0201 - accuracy: 0.9881 - val_loss: 0.2079 - val_accuracy: 0.9692 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0225 - accuracy: 0.9881 - val_loss: 0.1750 - val_accuracy: 0.9692 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0143 - accuracy: 0.9932 - val_loss: 0.1759 - val_accuracy: 0.9846 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0170 - accuracy: 0.9915 - val_loss: 0.1951 - val_accuracy: 0.9692 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.1974 - val_accuracy: 0.9692 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0221 - accuracy: 0.9915 - val_loss: 0.1764 - val_accuracy: 0.9846 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.1888 - val_accuracy: 0.9692 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.1918 - val_accuracy: 0.9692 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0154 - accuracy: 0.9932 - val_loss: 0.1916 - val_accuracy: 0.9692 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0099 - accuracy: 0.9949 - val_loss: 0.1972 - val_accuracy: 0.9692 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0194 - accuracy: 0.9915 - val_loss: 0.1924 - val_accuracy: 0.9692 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0137 - accuracy: 0.9983 - val_loss: 0.1965 - val_accuracy: 0.9692 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.1966 - val_accuracy: 0.9692 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1957 - val_accuracy: 0.9692 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.1970 - val_accuracy: 0.9692 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9692 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1939 - val_accuracy: 0.9692 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0124 - accuracy: 0.9983 - val_loss: 0.1957 - val_accuracy: 0.9692 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0127 - accuracy: 0.9983 - val_loss: 0.1956 - val_accuracy: 0.9692 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0130 - accuracy: 0.9949 - val_loss: 0.1938 - val_accuracy: 0.9692 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0119 - accuracy: 0.9949 - val_loss: 0.1935 - val_accuracy: 0.9692 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0119 - accuracy: 0.9966 - val_loss: 0.1939 - val_accuracy: 0.9692 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0096 - accuracy: 0.9966 - val_loss: 0.1930 - val_accuracy: 0.9692 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1924 - val_accuracy: 0.9692 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.1923 - val_accuracy: 0.9692 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.1923 - val_accuracy: 0.9692 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0094 - accuracy: 0.9949 - val_loss: 0.1923 - val_accuracy: 0.9692 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.1930 - val_accuracy: 0.9692 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 0.1926 - val_accuracy: 0.9692 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.1926 - val_accuracy: 0.9692 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0107 - accuracy: 0.9983 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0092 - accuracy: 0.9983 - val_loss: 0.1930 - val_accuracy: 0.9692 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 0.1933 - val_accuracy: 0.9692 - lr: 4.7018e-05\n",
            "Epoch 47/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.1929 - val_accuracy: 0.9692 - lr: 4.7018e-05\n",
            "Epoch 48/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 2.8211e-05\n",
            "Epoch 49/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9932 - val_loss: 0.1929 - val_accuracy: 0.9692 - lr: 2.8211e-05\n",
            "Epoch 50/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 2.8211e-05\n",
            "Epoch 51/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0140 - accuracy: 0.9966 - val_loss: 0.1927 - val_accuracy: 0.9692 - lr: 1.6927e-05\n",
            "Epoch 52/200\n",
            "74/74 [==============================] - 1s 9ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.1930 - val_accuracy: 0.9692 - lr: 1.6927e-05\n",
            "Epoch 53/200\n",
            "74/74 [==============================] - 1s 10ms/step - loss: 0.0146 - accuracy: 0.9932 - val_loss: 0.1933 - val_accuracy: 0.9692 - lr: 1.6927e-05\n",
            "65/65 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean, stdev\n",
        "print(mean(ACC_collecton),'±',stdev(ACC_collecton))\n",
        "print(mean(BACC_collecton),'±',stdev(BACC_collecton))\n",
        "print(mean(Sn_collecton),'±',stdev(Sn_collecton))\n",
        "print(mean(Sp_collecton),'±',stdev(Sp_collecton))\n",
        "print(mean(MCC_collecton),'±',stdev(MCC_collecton))\n",
        "print(mean(AUC_collecton),'±',stdev(AUC_collecton))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTi2x37MzsIY",
        "outputId": "ca6b5ab8-2d6c-4bea-d6e1-f4486481fa89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9679254079254079 ± 0.019494886009330258\n",
            "0.958523953171012 ± 0.032767742316934384\n",
            "0.9424664224664224 ± 0.06297559791341326\n",
            "0.9745814838756015 ± 0.017720334904160737\n",
            "0.8750757926127642 ± 0.07510612759780215\n",
            "0.9678133419625546 ± 0.04491376065460708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACC_collecton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "397sLYBohyh7",
        "outputId": "7cca9c4f-0da1-4f80-9bf3-b7da2713807b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9815950920245399]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model evaluation in test dataset"
      ],
      "metadata": {
        "id": "5JBlTA9shnQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "model, model_history = ESM_CNN(X_train, y_train, X_test , y_test)\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = model.predict(X_test,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "y_true = y_test    \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "# np.ravel() return a flatten 1D array\n",
        "TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "ACC_collecton.append(ACC)\n",
        "Sn_collecton.append(TP/(TP+FN))\n",
        "Sp_collecton.append(TN/(TN+FP))\n",
        "MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "MCC_collecton.append(MCC)\n",
        "BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "from sklearn.metrics import roc_auc_score\n",
        "AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "AUC_collecton.append(AUC)"
      ],
      "metadata": {
        "id": "KPwEv_WsnH6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92825f0-52ab-4c70-e8c4-bbf50264eca7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 1s 13ms/step - loss: 2.8669 - accuracy: 0.5400 - val_loss: 0.6995 - val_accuracy: 0.5000 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.7386 - accuracy: 0.5350 - val_loss: 0.7035 - val_accuracy: 0.5000 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.6398 - accuracy: 0.5650 - val_loss: 0.6973 - val_accuracy: 0.5000 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.5678 - accuracy: 0.7200 - val_loss: 0.7056 - val_accuracy: 0.5000 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.5266 - accuracy: 0.7100 - val_loss: 0.7220 - val_accuracy: 0.5000 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.4924 - accuracy: 0.7400 - val_loss: 0.7063 - val_accuracy: 0.5000 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4440 - accuracy: 0.7650 - val_loss: 0.6943 - val_accuracy: 0.5000 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3961 - accuracy: 0.8050 - val_loss: 0.6918 - val_accuracy: 0.5000 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3704 - accuracy: 0.8050 - val_loss: 0.6815 - val_accuracy: 0.5000 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3658 - accuracy: 0.7900 - val_loss: 0.6641 - val_accuracy: 0.6053 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.3357 - accuracy: 0.8200 - val_loss: 0.6444 - val_accuracy: 0.7632 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3265 - accuracy: 0.8250 - val_loss: 0.6262 - val_accuracy: 0.8158 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3000 - accuracy: 0.8200 - val_loss: 0.5917 - val_accuracy: 0.7895 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3074 - accuracy: 0.8250 - val_loss: 0.5613 - val_accuracy: 0.7895 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2620 - accuracy: 0.8700 - val_loss: 0.5326 - val_accuracy: 0.7895 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2617 - accuracy: 0.8500 - val_loss: 0.4946 - val_accuracy: 0.7895 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.3090 - accuracy: 0.8100 - val_loss: 0.4835 - val_accuracy: 0.7895 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2549 - accuracy: 0.8700 - val_loss: 0.4695 - val_accuracy: 0.7895 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2733 - accuracy: 0.8300 - val_loss: 0.4327 - val_accuracy: 0.8158 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2964 - accuracy: 0.8250 - val_loss: 0.4432 - val_accuracy: 0.7895 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2500 - accuracy: 0.8700 - val_loss: 0.4487 - val_accuracy: 0.7895 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2786 - accuracy: 0.8550 - val_loss: 0.4390 - val_accuracy: 0.8158 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2871 - accuracy: 0.8650 - val_loss: 0.4152 - val_accuracy: 0.8158 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2169 - accuracy: 0.8950 - val_loss: 0.4245 - val_accuracy: 0.8158 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2395 - accuracy: 0.8850 - val_loss: 0.4228 - val_accuracy: 0.8158 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2727 - accuracy: 0.8450 - val_loss: 0.4185 - val_accuracy: 0.8421 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2576 - accuracy: 0.8550 - val_loss: 0.4222 - val_accuracy: 0.8421 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2140 - accuracy: 0.8750 - val_loss: 0.4240 - val_accuracy: 0.8421 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2677 - accuracy: 0.8450 - val_loss: 0.4220 - val_accuracy: 0.8421 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2315 - accuracy: 0.8950 - val_loss: 0.4137 - val_accuracy: 0.8421 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2403 - accuracy: 0.8550 - val_loss: 0.4152 - val_accuracy: 0.8421 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2396 - accuracy: 0.8750 - val_loss: 0.4140 - val_accuracy: 0.8421 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2402 - accuracy: 0.8900 - val_loss: 0.4125 - val_accuracy: 0.8421 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2279 - accuracy: 0.8850 - val_loss: 0.4174 - val_accuracy: 0.8421 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2340 - accuracy: 0.8750 - val_loss: 0.4179 - val_accuracy: 0.8421 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2336 - accuracy: 0.8700 - val_loss: 0.4172 - val_accuracy: 0.8421 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2190 - accuracy: 0.8700 - val_loss: 0.4191 - val_accuracy: 0.8421 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2581 - accuracy: 0.8650 - val_loss: 0.4214 - val_accuracy: 0.8421 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2494 - accuracy: 0.8800 - val_loss: 0.4220 - val_accuracy: 0.8421 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2387 - accuracy: 0.8750 - val_loss: 0.4215 - val_accuracy: 0.8421 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2250 - accuracy: 0.8400 - val_loss: 0.4211 - val_accuracy: 0.8421 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2659 - accuracy: 0.8450 - val_loss: 0.4208 - val_accuracy: 0.8421 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.2347 - accuracy: 0.8800 - val_loss: 0.4208 - val_accuracy: 0.8421 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2177 - accuracy: 0.8750 - val_loss: 0.4207 - val_accuracy: 0.8421 - lr: 7.8364e-05\n",
            "Epoch 45/200\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.2245 - accuracy: 0.8750 - val_loss: 0.4209 - val_accuracy: 0.8421 - lr: 4.7018e-05\n",
            "Epoch 46/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2175 - accuracy: 0.8750 - val_loss: 0.4211 - val_accuracy: 0.8421 - lr: 4.7018e-05\n",
            "Epoch 47/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2338 - accuracy: 0.8800 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 4.7018e-05\n",
            "Epoch 48/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2479 - accuracy: 0.8500 - val_loss: 0.4216 - val_accuracy: 0.8421 - lr: 2.8211e-05\n",
            "Epoch 49/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2526 - accuracy: 0.8600 - val_loss: 0.4216 - val_accuracy: 0.8421 - lr: 2.8211e-05\n",
            "Epoch 50/200\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.2464 - accuracy: 0.8900 - val_loss: 0.4214 - val_accuracy: 0.8421 - lr: 2.8211e-05\n",
            "Epoch 51/200\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.2692 - accuracy: 0.8450 - val_loss: 0.4215 - val_accuracy: 0.8421 - lr: 1.6927e-05\n",
            "Epoch 52/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2500 - accuracy: 0.8600 - val_loss: 0.4214 - val_accuracy: 0.8421 - lr: 1.6927e-05\n",
            "Epoch 53/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2407 - accuracy: 0.8700 - val_loss: 0.4213 - val_accuracy: 0.8421 - lr: 1.6927e-05\n",
            "Epoch 54/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2364 - accuracy: 0.8700 - val_loss: 0.4214 - val_accuracy: 0.8421 - lr: 1.0156e-05\n",
            "Epoch 55/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2439 - accuracy: 0.8850 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 1.0156e-05\n",
            "Epoch 56/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2346 - accuracy: 0.8550 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 1.0156e-05\n",
            "Epoch 57/200\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.2392 - accuracy: 0.8500 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 6.0936e-06\n",
            "Epoch 58/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2360 - accuracy: 0.8800 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 6.0936e-06\n",
            "Epoch 59/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2257 - accuracy: 0.8750 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 6.0936e-06\n",
            "Epoch 60/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2162 - accuracy: 0.8850 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 3.6562e-06\n",
            "Epoch 61/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2564 - accuracy: 0.8550 - val_loss: 0.4211 - val_accuracy: 0.8421 - lr: 3.6562e-06\n",
            "Epoch 62/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2462 - accuracy: 0.8800 - val_loss: 0.4211 - val_accuracy: 0.8421 - lr: 3.6562e-06\n",
            "Epoch 63/200\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.2220 - accuracy: 0.8700 - val_loss: 0.4211 - val_accuracy: 0.8421 - lr: 2.1937e-06\n",
            "Epoch 64/200\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.8550 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 2.1937e-06\n",
            "Epoch 65/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2354 - accuracy: 0.8850 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 2.1937e-06\n",
            "Epoch 66/200\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.2336 - accuracy: 0.8550 - val_loss: 0.4212 - val_accuracy: 0.8421 - lr: 1.3162e-06\n",
            "38/38 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ACC_collecton[0])\n",
        "print(BACC_collecton[0])\n",
        "print(Sn_collecton[0])\n",
        "print(Sp_collecton[0])\n",
        "print(MCC_collecton[0])\n",
        "print(AUC_collecton[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF0RTvNSBcMl",
        "outputId": "e7f13f67-31ad-4ddc-b184-8934f48cbe1f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8421052631578947\n",
            "0.84593837535014\n",
            "0.8823529411764706\n",
            "0.8095238095238095\n",
            "0.6880329612324521\n",
            "0.9224376731301939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('BPP_tensorflow_model',save_format = 'tf') \n",
        "!zip -r /content/AMAP_alternative_tensorflow_model.zip /content/AMAP_alternative_tensorflow_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDNAw5DCUDHT",
        "outputId": "a4ab0d00-2616-42c6-a43a-0faba8190fcf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/AMAP_alternative_tensorflow_model/ (stored 0%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/variables/ (stored 0%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/variables/variables.data-00000-of-00001 (deflated 42%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/variables/variables.index (deflated 64%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/fingerprint.pb (stored 0%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/assets/ (stored 0%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/saved_model.pb (deflated 89%)\n",
            "updating: content/AMAP_alternative_tensorflow_model/keras_metadata.pb (deflated 90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE graph making"
      ],
      "metadata": {
        "id": "1G1yUX0bCZyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading datasset\n",
        "X_train_data_name = 'BBP_train_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X_test_data_name = 'BBP_test_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
        "X_train = np.array(X_train_data)\n",
        "X_test = np.array(X_test_data)\n",
        "# training dataset loading\n",
        "dataset = pd.read_excel('BBP_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# loading the y dataset for model development \n",
        "y_train = dataset['label']\n",
        "y_train = np.array(y_train) # transformed as np.array for CNN model\n",
        "# training dataset loading\n",
        "dataset = pd.read_excel('BBP_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "# loading the y dataset for model development \n",
        "y_test = dataset['label']\n",
        "y_test = np.array(y_test) # transformed as np.array for CNN model\n",
        "# normalize the X data range (just )\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)\n",
        "# concatenate the dataset\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.datasets import mnist\n",
        "from sklearn.datasets import load_iris\n",
        "from numpy import reshape\n",
        "import seaborn as sns\n",
        "import pandas as pd  \n",
        "tsne = TSNE(n_components=2, verbose=0, perplexity= 25, learning_rate='auto',n_iter = 5000,random_state=123)\n",
        "z = tsne.fit_transform(X) \n",
        "df = pd.DataFrame()\n",
        "df[\"comp-1\"] = z[:,0]\n",
        "df[\"comp-2\"] = z[:,1]\n",
        "y_new_label=[]\n",
        "for i in y:\n",
        "    if i == 0:\n",
        "        y_new_label.append('Active')\n",
        "    if i == 1:\n",
        "        y_new_label.append('Inactive')\n",
        "df[\"y\"] = y_new_label\n",
        "graph = sns.scatterplot(data=df, x=\"comp-1\", y=\"comp-2\", hue=y_new_label,\n",
        "                palette='BrBG_r', legend='full')\n",
        "graph_for_output = graph.get_figure()\n",
        "graph_for_output.savefig('11.BBP_t-SNE.png', dpi=300)\n",
        "df.to_excel('11.BBP_t-SNE.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "abadUCq3CeO3",
        "outputId": "67ea80d8-d380-4ddf-a827-d7cbaba87419"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQIWSlOUCwqb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}