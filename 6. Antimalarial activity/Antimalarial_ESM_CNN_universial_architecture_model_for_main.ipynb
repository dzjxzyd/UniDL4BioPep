{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### requirements for the following codings\n"
      ],
      "metadata": {
        "id": "95NTckuFZZzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### packages required \n",
        "!pip install fair-esm \n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "UO71IBS6ZgZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc472b55-f2b4-451d-c269-a66d635c1db6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
        "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
        "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt"
      ],
      "metadata": {
        "id": "m91cA0H5w_eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def esm_embeddings(peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long, \n",
        "  #         or you have too many sequences for transformation in a single converting, \n",
        "  #         you conputer might automatically kill the job.\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  # load the model\n",
        "  # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
        "  model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "  batch_converter = alphabet.get_batch_converter()\n",
        "  model.eval()  # disables dropout for deterministic results\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the embedding results of the whole data set\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      results = model(batch_tokens, repr_layers=[6], return_contacts=True)  \n",
        "  token_representations = results[\"representations\"][6]\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  return embeddings_results"
      ],
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data loading and embeddings (main dataset)"
      ],
      "metadata": {
        "id": "RddxugbsdR1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "n6NOFoREw-40"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('AMAP_train_main.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "sequence_list_new = []\n",
        "y_train = []\n",
        "for i in range(len(sequence_list)):\n",
        "  if len(sequence_list[i]) < 100:\n",
        "    sequence_list_new.append(sequence_list[i])\n",
        "    y_train.append(dataset['label'][i])\n"
      ],
      "metadata": {
        "id": "7f-rZMcgMrWU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('AMAP_train_main.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('AMAP_train_main_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_train = dataset['label']\n",
        "y_train = np.array(y_train) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "LNlD8pvizH84"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset loading\n",
        "dataset = pd.read_excel('AMAP_test_main.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('AMAP_test_main_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_test = dataset['label']\n",
        "y_test = np.array(y_test) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "U7jxoIsCw8dW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the dataset \n",
        "X_train_data_name = 'AMAP_train_main_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_test_data_name = 'AMAP_test_main_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_train = np.array(X_train_data)\n",
        "X_test = np.array(X_test_data)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Xk13-JbBXAph"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the dimension of the dataset before model development\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HubTATKXslKw",
        "outputId": "dbdb3ca9-11e7-4207-912e-a20e0b67dc22"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1819, 320)\n",
            "(455, 320)\n",
            "(1819, 1)\n",
            "(455,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data loading and embeddings (alternative dataset)"
      ],
      "metadata": {
        "id": "7qxqYf-d_dCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LXftqHY1_iEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('AMAP_train_alternative.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('AMAP_train_alternative_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_train = dataset['label']\n",
        "y_train = np.array(y_train) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "VYDJ5sxJ_ilc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test dataset loading\n",
        "dataset = pd.read_excel('AMAP_test_alternative.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence'] \n",
        "\n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    format_seq = [seq,seq] # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple(format_seq)\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "\n",
        "embeddings_results.to_csv('AMAP_test_alternative_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# loading the y dataset for model development \n",
        "y_test = dataset['label']\n",
        "y_test = np.array(y_test) # transformed as np.array for CNN model"
      ],
      "metadata": {
        "id": "dH3XGmWJ_ifc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the dataset \n",
        "X_train_data_name = 'AMAP_train_alternative_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_train_data = pd.read_csv(X_train_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_test_data_name = 'AMAP_test_alternative_esm2_t6_8M_UR50D_unified_320_dimension.csv'\n",
        "X_test_data = pd.read_csv(X_test_data_name,header=0, index_col = 0,delimiter=',')\n",
        "\n",
        "X_train = np.array(X_train_data)\n",
        "X_test = np.array(X_test_data)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range \n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "I_QVh1hA_ib4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_data"
      ],
      "metadata": {
        "id": "sfqfI_vZdN4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the dimension of the dataset before model development\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk831Klh_iXF",
        "outputId": "700d82d2-4c9a-4e7e-81c8-94bf525ece06"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1819, 320)\n",
            "(455, 320)\n",
            "(1819, 1)\n",
            "(163,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architecture"
      ],
      "metadata": {
        "id": "U3Fagh9Iw83q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ESM_CNN(X_train, y_train, X_test, y_test):\n",
        "  from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D,Conv1D\n",
        "  from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, AveragePooling1D, MaxPooling1D\n",
        "  from keras.models import Sequential,Model\n",
        "  from keras.optimizers import SGD\n",
        "  from keras.callbacks import ModelCheckpoint,LearningRateScheduler, EarlyStopping\n",
        "  import keras\n",
        "  from keras import backend as K\n",
        "  inputShape=(320,1)\n",
        "  input = Input(inputShape)\n",
        "  x = Conv1D(128,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Conv1D(32,(3),strides = (1),name='layer_conv2',padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling1D((2), name='MaxPool2',padding=\"same\")(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "  model = Model(inputs = input,outputs = x,name='Predict')\n",
        "  # define SGD optimizer\n",
        "  momentum = 0.5\n",
        "  sgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False)\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
        "  # learning deccay setting\n",
        "  import math\n",
        "  def step_decay(epoch): # gradually decrease the learning rate \n",
        "      initial_lrate=0.1\n",
        "      drop=0.6\n",
        "      epochs_drop = 3.0\n",
        "      lrate= initial_lrate * math.pow(drop,    # math.pow base raised to a power\n",
        "            math.floor((1+epoch)/epochs_drop)) # math.floor Round numbers down to the nearest integer\n",
        "      return lrate\n",
        "  lrate = LearningRateScheduler(step_decay)\n",
        "\n",
        "  # early stop setting\n",
        "  early_stop = EarlyStopping(monitor='val_accuracy', patience = 40,restore_best_weights = True)\n",
        "\n",
        "  # summary the callbacks_list\n",
        "  callbacks_list = [ lrate , early_stop]\n",
        "\n",
        "  model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "                            epochs=200,callbacks=callbacks_list,batch_size = 8, verbose=1)\n",
        "  return model, model_history"
      ],
      "metadata": {
        "id": "b0QeK6-Cg_cv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10-fold cross validation"
      ],
      "metadata": {
        "id": "sws_G8h08tuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing 10-fold cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "k = 10 \n",
        "kf = KFold(n_splits=k, shuffle = True, random_state=1)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "\n",
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "\n",
        "for train_index , test_index in kf.split(y_train):\n",
        "    X_train_CV , X_valid_CV = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
        "    y_train_CV , y_valid_CV = y_train.iloc[train_index] , y_train.iloc[test_index]\n",
        "    model, model_history = ESM_CNN(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
        "    # confusion matrix \n",
        "    predicted_class= []\n",
        "    predicted_protability = model.predict(X_valid_CV,batch_size=1)\n",
        "    for i in range(predicted_protability.shape[0]):\n",
        "      index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "      predicted_class.append(index)\n",
        "    predicted_class = np.array(predicted_class)\n",
        "    y_true = y_valid_CV    \n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import math\n",
        "    # np.ravel() return a flatten 1D array\n",
        "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "    ACC_collecton.append(ACC)\n",
        "    Sn_collecton.append(TP/(TP+FN))\n",
        "    Sp_collecton.append(TN/(TN+FP))\n",
        "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "    MCC_collecton.append(MCC)\n",
        "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    AUC = roc_auc_score(y_valid_CV, predicted_protability[:,1])\n",
        "    AUC_collecton.append(AUC)\n"
      ],
      "metadata": {
        "id": "iFGZ88goj6u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean, stdev\n",
        "print(mean(ACC_collecton),'±',stdev(ACC_collecton))\n",
        "print(mean(BACC_collecton),'±',stdev(BACC_collecton))\n",
        "print(mean(Sn_collecton),'±',stdev(Sn_collecton))\n",
        "print(mean(Sp_collecton),'±',stdev(Sp_collecton))\n",
        "print(mean(MCC_collecton),'±',stdev(MCC_collecton))\n",
        "print(mean(AUC_collecton),'±',stdev(AUC_collecton))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTi2x37MzsIY",
        "outputId": "5cdef815-5915-4054-f27a-9b76fd95e1a7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9730647805233441 ± 0.016056815365839562\n",
            "0.9530839977101483 ± 0.0474693330016521\n",
            "0.9313888888888889 ± 0.0962611224934745\n",
            "0.9747791065314076 ± 0.01709568816060024\n",
            "0.7436013348063777 ± 0.09753319906980065\n",
            "0.9164483992457524 ± 0.049252339327419625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model evaluation in test dataset"
      ],
      "metadata": {
        "id": "5JBlTA9shnQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "model, model_history = ESM_CNN(X_train, y_train, X_test , y_test)\n",
        "# confusion matrix \n",
        "predicted_class= []\n",
        "predicted_protability = model.predict(X_test,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n",
        "y_true = y_test    \n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "# np.ravel() return a flatten 1D array\n",
        "TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "ACC_collecton.append(ACC)\n",
        "Sn_collecton.append(TP/(TP+FN))\n",
        "Sp_collecton.append(TN/(TN+FP))\n",
        "MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "MCC_collecton.append(MCC)\n",
        "BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "from sklearn.metrics import roc_auc_score\n",
        "AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "AUC_collecton.append(AUC)"
      ],
      "metadata": {
        "id": "KPwEv_WsnH6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902d8939-fe1d-4b99-9945-7f44a94f287f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "228/228 [==============================] - 3s 10ms/step - loss: 0.8325 - accuracy: 0.9434 - val_loss: 0.1987 - val_accuracy: 0.9451 - lr: 0.1000\n",
            "Epoch 2/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.1299 - accuracy: 0.9599 - val_loss: 0.1107 - val_accuracy: 0.9780 - lr: 0.1000\n",
            "Epoch 3/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.1020 - accuracy: 0.9681 - val_loss: 0.1100 - val_accuracy: 0.9758 - lr: 0.0600\n",
            "Epoch 4/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0885 - accuracy: 0.9714 - val_loss: 0.1016 - val_accuracy: 0.9802 - lr: 0.0600\n",
            "Epoch 5/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0801 - accuracy: 0.9747 - val_loss: 0.1030 - val_accuracy: 0.9802 - lr: 0.0600\n",
            "Epoch 6/200\n",
            "228/228 [==============================] - 3s 11ms/step - loss: 0.0720 - accuracy: 0.9764 - val_loss: 0.1018 - val_accuracy: 0.9780 - lr: 0.0360\n",
            "Epoch 7/200\n",
            "228/228 [==============================] - 3s 12ms/step - loss: 0.0660 - accuracy: 0.9769 - val_loss: 0.1129 - val_accuracy: 0.9758 - lr: 0.0360\n",
            "Epoch 8/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0615 - accuracy: 0.9808 - val_loss: 0.1102 - val_accuracy: 0.9736 - lr: 0.0360\n",
            "Epoch 9/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0502 - accuracy: 0.9819 - val_loss: 0.1088 - val_accuracy: 0.9758 - lr: 0.0216\n",
            "Epoch 10/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0519 - accuracy: 0.9841 - val_loss: 0.1168 - val_accuracy: 0.9780 - lr: 0.0216\n",
            "Epoch 11/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0434 - accuracy: 0.9852 - val_loss: 0.1188 - val_accuracy: 0.9736 - lr: 0.0216\n",
            "Epoch 12/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0431 - accuracy: 0.9841 - val_loss: 0.1171 - val_accuracy: 0.9758 - lr: 0.0130\n",
            "Epoch 13/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.1216 - val_accuracy: 0.9758 - lr: 0.0130\n",
            "Epoch 14/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0408 - accuracy: 0.9857 - val_loss: 0.1195 - val_accuracy: 0.9736 - lr: 0.0130\n",
            "Epoch 15/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0393 - accuracy: 0.9868 - val_loss: 0.1171 - val_accuracy: 0.9736 - lr: 0.0078\n",
            "Epoch 16/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0385 - accuracy: 0.9863 - val_loss: 0.1221 - val_accuracy: 0.9780 - lr: 0.0078\n",
            "Epoch 17/200\n",
            "228/228 [==============================] - 3s 13ms/step - loss: 0.0380 - accuracy: 0.9868 - val_loss: 0.1235 - val_accuracy: 0.9758 - lr: 0.0078\n",
            "Epoch 18/200\n",
            "228/228 [==============================] - 4s 17ms/step - loss: 0.0391 - accuracy: 0.9863 - val_loss: 0.1207 - val_accuracy: 0.9758 - lr: 0.0047\n",
            "Epoch 19/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0314 - accuracy: 0.9874 - val_loss: 0.1276 - val_accuracy: 0.9758 - lr: 0.0047\n",
            "Epoch 20/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0335 - accuracy: 0.9879 - val_loss: 0.1246 - val_accuracy: 0.9758 - lr: 0.0047\n",
            "Epoch 21/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.1243 - val_accuracy: 0.9758 - lr: 0.0028\n",
            "Epoch 22/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0295 - accuracy: 0.9874 - val_loss: 0.1267 - val_accuracy: 0.9758 - lr: 0.0028\n",
            "Epoch 23/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0362 - accuracy: 0.9863 - val_loss: 0.1267 - val_accuracy: 0.9758 - lr: 0.0028\n",
            "Epoch 24/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0301 - accuracy: 0.9874 - val_loss: 0.1270 - val_accuracy: 0.9758 - lr: 0.0017\n",
            "Epoch 25/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0284 - accuracy: 0.9901 - val_loss: 0.1293 - val_accuracy: 0.9758 - lr: 0.0017\n",
            "Epoch 26/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0297 - accuracy: 0.9907 - val_loss: 0.1304 - val_accuracy: 0.9758 - lr: 0.0017\n",
            "Epoch 27/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0335 - accuracy: 0.9890 - val_loss: 0.1302 - val_accuracy: 0.9758 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.1305 - val_accuracy: 0.9758 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0311 - accuracy: 0.9901 - val_loss: 0.1294 - val_accuracy: 0.9758 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0257 - accuracy: 0.9907 - val_loss: 0.1303 - val_accuracy: 0.9758 - lr: 6.0466e-04\n",
            "Epoch 31/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0297 - accuracy: 0.9890 - val_loss: 0.1317 - val_accuracy: 0.9758 - lr: 6.0466e-04\n",
            "Epoch 32/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0301 - accuracy: 0.9896 - val_loss: 0.1310 - val_accuracy: 0.9758 - lr: 6.0466e-04\n",
            "Epoch 33/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0326 - accuracy: 0.9890 - val_loss: 0.1309 - val_accuracy: 0.9736 - lr: 3.6280e-04\n",
            "Epoch 34/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0256 - accuracy: 0.9907 - val_loss: 0.1313 - val_accuracy: 0.9736 - lr: 3.6280e-04\n",
            "Epoch 35/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0271 - accuracy: 0.9890 - val_loss: 0.1322 - val_accuracy: 0.9736 - lr: 3.6280e-04\n",
            "Epoch 36/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0289 - accuracy: 0.9896 - val_loss: 0.1312 - val_accuracy: 0.9758 - lr: 2.1768e-04\n",
            "Epoch 37/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0334 - accuracy: 0.9863 - val_loss: 0.1313 - val_accuracy: 0.9736 - lr: 2.1768e-04\n",
            "Epoch 38/200\n",
            "228/228 [==============================] - 2s 9ms/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 0.1308 - val_accuracy: 0.9758 - lr: 2.1768e-04\n",
            "Epoch 39/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0273 - accuracy: 0.9901 - val_loss: 0.1313 - val_accuracy: 0.9758 - lr: 1.3061e-04\n",
            "Epoch 40/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.1304 - val_accuracy: 0.9758 - lr: 1.3061e-04\n",
            "Epoch 41/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0309 - accuracy: 0.9874 - val_loss: 0.1303 - val_accuracy: 0.9758 - lr: 1.3061e-04\n",
            "Epoch 42/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0276 - accuracy: 0.9890 - val_loss: 0.1312 - val_accuracy: 0.9758 - lr: 7.8364e-05\n",
            "Epoch 43/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0307 - accuracy: 0.9890 - val_loss: 0.1311 - val_accuracy: 0.9758 - lr: 7.8364e-05\n",
            "Epoch 44/200\n",
            "228/228 [==============================] - 2s 10ms/step - loss: 0.0278 - accuracy: 0.9879 - val_loss: 0.1317 - val_accuracy: 0.9736 - lr: 7.8364e-05\n",
            "455/455 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ACC_collecton[0])\n",
        "print(BACC_collecton[0])\n",
        "print(Sn_collecton[0])\n",
        "print(Sp_collecton[0])\n",
        "print(MCC_collecton[0])\n",
        "print(AUC_collecton[0])"
      ],
      "metadata": {
        "id": "nOkHijttl10O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1180b323-8c26-42df-d922-16a7b23b34f4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9802197802197802\n",
            "0.9896788990825688\n",
            "1.0\n",
            "0.9793577981651376\n",
            "0.8152080839782481\n",
            "0.920541987286718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('AMAP_main_tensorflow_model',save_format = 'tf') \n",
        "!zip -r /content/AMAP_main_tensorflow_model.zip /content/AMAP_main_tensorflow_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDNAw5DCUDHT",
        "outputId": "9c832510-7ef1-42f9-a638-214e2b2887fe"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/AMAP_main_tensorflow_model/ (stored 0%)\n",
            "  adding: content/AMAP_main_tensorflow_model/variables/ (stored 0%)\n",
            "  adding: content/AMAP_main_tensorflow_model/variables/variables.index (deflated 64%)\n",
            "  adding: content/AMAP_main_tensorflow_model/variables/variables.data-00000-of-00001 (deflated 41%)\n",
            "  adding: content/AMAP_main_tensorflow_model/saved_model.pb (deflated 88%)\n",
            "  adding: content/AMAP_main_tensorflow_model/assets/ (stored 0%)\n",
            "  adding: content/AMAP_main_tensorflow_model/keras_metadata.pb (deflated 89%)\n"
          ]
        }
      ]
    }
  ]
}